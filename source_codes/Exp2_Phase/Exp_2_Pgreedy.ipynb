{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"kGlD1GYTOASy"},"outputs":[],"source":["pip install pennylane"]},{"cell_type":"markdown","metadata":{"id":"cfi1Y1sisIjT"},"source":["# Helpers"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VuiTicDYsKDy"},"outputs":[],"source":["import numpy as np\n","\n","class GaussianProcessRegressor:\n","    \"\"\"\n","    Gaussian Process Regressor for d-dimensional inputs.\n","    Returns both predictive mean and std if requested.\n","    \"\"\"\n","    def __init__(self, kernel, alpha=1e-5):\n","        self.kernel = kernel\n","        self.alpha = alpha\n","        self.X_train = None\n","        self.y_train = None\n","        self.K_inv = None\n","\n","    def fit(self, X, y):\n","        X = np.array(X)\n","        y = np.array(y)\n","        self.X_train = X\n","        self.y_train = y\n","        K = self.kernel(X, X)\n","        K += self.alpha * np.eye(len(X))\n","        try:\n","          self.K_inv = np.linalg.inv(K)\n","        except:\n","          print(\"error\")\n","          self.K_inv = np.linalg.pinv(K)\n","\n","    def predict(self, X_test, return_std=False):\n","\n","        X_test = np.array(X_test)\n","        K_star = self.kernel(X_test, self.X_train)\n","        y_mean = K_star @ (self.K_inv @ self.y_train)\n","\n","        if return_std:\n","            K_star_star = self.kernel(X_test, X_test)\n","            cov = K_star_star - K_star @ self.K_inv @ K_star.T\n","            var = np.diag(cov)\n","            var = np.maximum(var, 0.0)\n","            y_std = np.sqrt(var)\n","            return y_mean, y_std\n","        else:\n","            return y_mean\n","\n","\n","\n","from itertools import product\n","\n","def build_grid(bounds, n_grid):\n","    \"\"\"\n","    Build a uniform grid of points within the given 'bounds'.\n","\n","    Parameters\n","    ----------\n","    bounds : list of (low, high) for each dimension (length d)\n","    n_grid : int\n","        Number of grid points per dimension.\n","\n","    Returns\n","    -------\n","    X_grid : np.ndarray of shape (n_grid^d, d)\n","        All points in the grid.\n","    \"\"\"\n","    # For each dimension, create an array of n_grid points from low to high\n","    axes = [np.linspace(low, high, n_grid) for (low, high) in bounds]\n","    # Cartesian product of all axes\n","    # e.g. for d=2, we get all pairs (x,y); for d=3, all (x,y,z), etc.\n","    mesh = list(product(*axes))  # a list of d-tuples\n","    return np.array(mesh)\n","\n","\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from itertools import product\n","from scipy.stats import norm\n","import random\n","\n","\n","def gp_ucb_nd(\n","    f,                 # black-box reward function, shape: (n, d) -> (n,)\n","    gp,                # a GaussianProcessRegressor instance (fit, predict)\n","    bounds,            # list of (low, high) for each dimension\n","    n_iter=10,         # number of UCB iterations\n","    init_points=3,     # number of initial random samples\n","    n_grid=50,         # number of grid points per dimension to search for UCB max\n","    beta_func=None,    # a callable or None => we define a default\n","    random_state=None,\n","    error = 1,\n","    verbose=False\n","):\n","    \"\"\"\n","    Runs GP-UCB for a multi-armed bandit / Bayesian optimization problem.\n","\n","    Parameters\n","    ----------\n","    f : callable\n","        The unknown reward function (black-box).\n","        Input shape (n, d) -> (n,) for n points in dimension d.\n","    gp : GaussianProcessRegressor\n","        Surrogate GP model implementing:\n","          - gp.fit(X, y)\n","          - gp.predict(X, return_std=True) -> (mean, std)\n","    bounds : list of (low, high)\n","        Domain bounding box in d dimensions.\n","    n_iter : int\n","        Number of GP-UCB rounds.\n","    init_points : int\n","        How many random points to sample initially.\n","    n_grid : int\n","        Grid resolution for selecting x_t = argmax_x [mu_{t-1}(x) + sqrt(beta_t)*sigma_{t-1}(x)].\n","    beta_func : callable or None\n","        If None, we define a simple default that grows with t.  Otherwise,\n","        something like:  beta_func(t) -> float\n","    random_state : int or None\n","        For reproducibility.\n","    verbose : bool\n","        Print iteration details if True.\n","\n","    Returns\n","    -------\n","    regrets : ndarray of shape (n_iter + init_points,)\n","        Cumulative regret at each iteration (the sum over t=1..T of [f(x*) - f(x_t)]).\n","    X_samples : ndarray of shape (n_iter + init_points, d)\n","    Y_samples : ndarray of shape (n_iter + init_points,)\n","\n","    Notes\n","    -----\n","    - We assume we can approximate the global max f(x*) by a dense grid search\n","      once, for regret calculation.  In a real bandit scenario, you might not\n","      know x*, but for synthetic tests or known benchmark functions, we do.\n","    - The domain is searched by a grid of size (n_grid^d).  This is only feasible\n","      for small d or moderate n_grid.\n","    \"\"\"\n","\n","    if random_state is not None:\n","        np.random.seed(random_state)\n","\n","    d = len(bounds)\n","\n","    # 1) Create a dense grid to:\n","    #    - approximate x* (the global maximizer)\n","    #    - search for the UCB argmax each iteration\n","    X_grid = build_grid(bounds, n_grid=n_grid)  # shape (n_grid^d, d)\n","\n","    # 2) (Optional) approximate the global maximum for regret calculation\n","    #    We take the best among the same grid points to get x_star\n","    Y_grid = f(X_grid)\n","    idx_best = np.argmax(Y_grid)\n","    x_star = X_grid[idx_best]\n","    f_star = Y_grid[idx_best]  # approximate global max value\n","    if f_star == 0:\n","      print(\"f_star is zero???\")\n","\n","    # 3) Helper to define a default beta_t if user didn't supply one\n","    if beta_func is None:\n","        # E.g. a common choice for finite domain or small bounding\n","        # can be something like: 2 ln(t^2 * pi^2 / (6 delta))\n","        # but we just do a simpler scaling of log t for demonstration.\n","        def beta_func(t):\n","            return 2.0 * np.log(1000 * t**2 * 10 / 6 / 0.1 )   # delta = 0.1\n","    # else user supplies something like beta_func(t) => some formula\n","\n","    # 4) Initialize data by sampling 'init_points' random points\n","    def sample_random(n):\n","        return np.array([\n","            X_grid[random.sample(range(1, 10),1)[0]]\n","            for _ in range(n)\n","        ])\n","\n","    X_samples = sample_random(init_points)  # shape (init_points, d)\n","    Y_samples = f(X_samples)               # shape (init_points,)\n","\n","    # 5) We'll keep track of cumulative regrets\n","    regrets = np.zeros(n_iter + init_points)\n","    # First 'init_points' regrets are computed from those random picks\n","    # Evaluate the regret at each step\n","    cum_regret = 0.0\n","    for i in range(init_points):\n","        cum_regret += (f_star - Y_samples[i])\n","        regrets[i] = cum_regret\n","\n","    # 6) Main loop of GP-UCB\n","    log_sum_for_beta = 0\n","    for step in range(n_iter):\n","        #print(step)\n","        t = init_points + step + 1  # total iteration index (1-based)\n","\n","        # Fit GP on current data\n","        gp.fit(X_samples, Y_samples)\n","\n","        # Compute mean & std on the entire grid\n","        mu_grid, std_grid = gp.predict(X_grid, return_std=True)\n","\n","        # Define current beta_t\n","        beta_t = 0.1 * (2*np.log(20)+log_sum_for_beta) ** 0.5 + 3\n","\n","        #beta_func(t)\n","\n","        # UCB = mu + sqrt(beta_t) * std\n","        ucb_values = mu_grid + (beta_t + error * np.sqrt(t) ) * std_grid\n","\n","        # if step % 10 == 0:\n","        #   print(step)\n","        #   print(mu_grid[:10], std_grid[:10])\n","        #   print(ucb_values[:10])\n","        #   #print(np.max(ucb_values))\n","        #   print(\"==========\")\n","\n","        # Argmax on the grid\n","        idx_next = np.argmax(ucb_values)\n","        x_next = X_grid[idx_next].reshape(1, -1)\n","\n","        log_sum_for_beta += np.log(1 + std_grid[idx_next])\n","\n","        #print(beta_t + error * np.sqrt(t) )\n","        #print(std_grid[idx_next])\n","\n","        # Evaluate the unknown function f (bandit feedback)\n","        np.random.seed(None)\n","        noise = np.random.normal(0, 0.1)\n","        y_true = f(x_next)\n","        y_next = y_true + noise\n","        #print(noise)\n","\n","        # Update data\n","        X_samples = np.vstack([X_samples, x_next])\n","        Y_samples = np.concatenate([Y_samples, y_next])\n","\n","        # Update cumulative regret\n","        cum_regret += (f_star - y_true[0])\n","        regrets[init_points + step] = cum_regret\n","\n","        if verbose:\n","            print(f\"Iteration {step+1}/{n_iter}, t={t}, beta={beta_t:.3f}, \"\n","                  f\"x_next={x_next[0]}, f(x)={y_next[0]:.4f}, UCB={ucb_values[idx_next]:.4f}, \"\n","                  f\"CumReg={cum_regret:.4f}\")\n","\n","    return regrets, X_samples, Y_samples\n","\n","\n","\n","\n","\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from tqdm import trange\n","\n","def run_multiple_experiments_gp_ucb(\n","    f,\n","    gp,\n","    bounds,\n","    n_runs=5,\n","    n_iter=10,\n","    init_points=3,\n","    n_grid=50,\n","    beta_func=None,\n","    random_state=None,\n","    error = 1,\n","    verbose=False\n","):\n","    \"\"\"\n","    Runs the GP-UCB experiment 'n_runs' times, each time creating a new GP instance.\n","    Returns the average cumulative regret across runs.\n","\n","    Parameters\n","    ----------\n","    f : callable\n","        The black-box reward function.\n","    gp_class_factory : callable\n","        A function that, when called, returns a *new* untrained GaussianProcessRegressor\n","        (or similar). For example:\n","          lambda: GaussianProcessRegressor(kernel=RBF(...), alpha=..., optimizer=None)\n","        We need a fresh GP for each run, so that each run is independent.\n","    bounds : list of (low, high)\n","        Domain bounding box.\n","    n_runs : int\n","        Number of independent runs to average over.\n","    n_iter : int\n","        Number of GP-UCB rounds (not counting the init_points).\n","    init_points : int\n","        Number of random initial points in each run.\n","    n_grid : int\n","        Grid resolution for argmax search in each run.\n","    beta_func : callable or None\n","        If None, use a default log-based. Otherwise a function beta_func(t) -> float.\n","    random_state : int or None\n","        For reproducibility. If set, seeds the first run's RNG, then subsequent runs\n","        will shift the seed.\n","    verbose : bool\n","        Whether to print details for each run.\n","\n","    Returns\n","    -------\n","    avg_regret : ndarray of shape (n_iter + init_points,)\n","        The pointwise average of the cumulative regret across runs.\n","    regrets_all : ndarray of shape (n_runs, n_iter + init_points)\n","        The individual run's cumulative-regret curves.\n","    \"\"\"\n","\n","    from copy import deepcopy\n","\n","    # If we want reproducibility, set base seed\n","    base_seed = random_state if random_state is not None else None\n","\n","    # We'll store the regret curve for each run here\n","    regrets_all = []\n","\n","    for i in trange(n_runs, desc=\"GP-UCB Experiments\"):\n","        # For each run, optionally shift the seed\n","        if base_seed is not None:\n","            # shift by i to get distinct seeds\n","            np.random.seed(base_seed + i)\n","\n","        # Create a fresh GP instance\n","        gp_model = gp\n","\n","        # Import or copy the gp_ucb_nd from your previous code snippet:\n","        regrets, X_samples, Y_samples = gp_ucb_nd(\n","            f=f,\n","            gp=gp_model,\n","            bounds=bounds,\n","            n_iter=n_iter,\n","            init_points=init_points,\n","            n_grid=n_grid,\n","            beta_func=beta_func,\n","            random_state=None,  # we've set the seed externally\n","            error = error,\n","            verbose=(verbose and i == 0)  # only verbose in the 1st run, e.g.\n","        )\n","        print(regrets)\n","        regrets_all.append(regrets)\n","\n","    regrets_all = np.array(regrets_all)  # shape (n_runs, n_steps)\n","    avg_regret = regrets_all.mean(axis=0)\n","    std_regret = regrets_all.std(axis=0)\n","    return avg_regret, regrets_all, std_regret\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"kd61EOQcslbm"},"source":["## build kernels"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rm6Ir-3-CW5l"},"outputs":[],"source":["import pennylane as qml\n","from pennylane import numpy as np\n","import matplotlib.pyplot as plt\n","import scipy.sparse.linalg as spla\n","\n","##############################################################################\n","# 1) BINARY LABEL => label=1 if (above_l1 & below_l2), else label=0\n","##############################################################################\n","\n","def binary_label_function(J1, J2):\n","    below_l1= (J2 < -1 - J1)\n","    above_l1= (J2 > -1 - J1)\n","    above_l2= (J2 >  J1 - 1)\n","    below_l2= (J2 <  J1 - 1)\n","    # ignoring l3=1\n","\n","    if (above_l1 and below_l2):\n","        return 1\n","    else:\n","        return 0\n","\n","##############################################################################\n","# 2) Cluster Hamiltonian => ground state\n","##############################################################################\n","\n","def build_cluster_hamiltonian(n_qubits, J1, J2):\n","    def mod_idx(k):\n","        return k % n_qubits\n","\n","    coeffs=[]\n","    ops=[]\n","    for j in range(n_qubits):\n","        ops.append(qml.PauliZ(j))\n","        coeffs.append(1.0)\n","\n","        ops.append(qml.PauliX(j) @ qml.PauliX(mod_idx(j+1)))\n","        coeffs.append(-J1)\n","\n","        ops.append(qml.PauliX(mod_idx(j-1)) @ qml.PauliZ(j) @ qml.PauliX(mod_idx(j+1)))\n","        coeffs.append(-J2)\n","\n","    return qml.Hamiltonian(coeffs, ops)\n","\n","def ground_state_exact(n_qubits, J1, J2):\n","    H= build_cluster_hamiltonian(n_qubits, J1, J2)\n","    sm= H.sparse_matrix()\n","    vals, vecs= spla.eigsh(sm, k=1, which='SA')\n","    return vecs[:,0]\n","\n","##############################################################################\n","# 3) 2-layer circuit => measure qubit0 => [p0,p1]\n","##############################################################################\n","\n","def multi_layer_circuit(params, n_qubits, data_vec, n_layers=2):\n","    qml.StatePrep(data_vec, wires=range(n_qubits))\n","\n","    offset=0\n","    for _ in range(n_layers):\n","        # single-qubit\n","        for i in range(n_qubits):\n","            rx= params[offset+2*i]\n","            rz= params[offset+2*i+1]\n","            qml.RX(rx, wires=i)\n","            qml.RZ(rz, wires=i)\n","        offset+= 2*n_qubits\n","\n","        # ring of cnot\n","        for i in range(n_qubits):\n","            qml.CNOT(wires=[i,(i+1)%n_qubits])\n","\n","    return qml.probs(wires=[0])  # => [p0,p1]\n","\n","def create_classifier_qnode(n_qubits, n_layers=2):\n","    dev= qml.device(\"default.qubit\", wires=n_qubits)\n","    total_params= 2*n_qubits*n_layers\n","\n","    @qml.qnode(dev)\n","    def classifier(params, data_vec):\n","        return multi_layer_circuit(params, n_qubits, data_vec, n_layers=n_layers)\n","\n","    return classifier, total_params\n","\n","##############################################################################\n","# 4) cost => 1 - average p_correct => label in {0,1}\n","##############################################################################\n","\n","def empirical_risk(params, classifier_circuit, states, labels):\n","    N= len(states)\n","    sum_p= 0.0\n","    for i in range(N):\n","        lbl= labels[i]\n","        p2= classifier_circuit(params, states[i])\n","        sum_p+= p2[int(lbl)]\n","    return sum_p/N\n","\n","def cost_fn(params, classifier_circuit, states, labels):\n","    return 1.0 - empirical_risk(params, classifier_circuit, states, labels)\n","\n","def accuracy(params, classifier_circuit, states, labels):\n","    correct= 0\n","    for s,lbl in zip(states, labels):\n","        p2= classifier_circuit(params, s)\n","        pred_idx= int(np.argmax(p2))\n","        if pred_idx==lbl:\n","            correct+=1\n","    return correct/ len(states)\n","\n","##############################################################################\n","# 5) data grid => STILL returns ground states in `states`,\n","#    used for training.  We'll keep this the same.\n","##############################################################################\n","\n","def generate_data_grid(n_qubits, Nx, Ny, j1min, j1max, j2min, j2max, label_func):\n","    j1_vals= np.linspace(j1min, j1max, Nx)\n","    j2_vals= np.linspace(j2min, j2max, Ny)\n","    states=[]\n","    labels=[]\n","    j1_collect=[]\n","    j2_collect=[]\n","    for j1 in j1_vals:\n","        for j2 in j2_vals:\n","            st= ground_state_exact(n_qubits, j1, j2)\n","            lab= label_func(j1,j2)\n","            states.append(st)\n","            labels.append(lab)\n","            j1_collect.append(j1)\n","            j2_collect.append(j2)\n","\n","    states= np.array(states, dtype=object)\n","    labels= np.array(labels, dtype=int)\n","    j1_collect= np.array(j1_collect)\n","    j2_collect= np.array(j2_collect)\n","    return states, labels, j1_collect, j2_collect\n","\n","##############################################################################\n","# 6) QNode that returns a partial or full-wire density matrix, given ground state\n","##############################################################################\n","\n","def create_density_qnode(n_qubits, n_layers=2, wires_to_measure=None):\n","    if wires_to_measure is None:\n","        wires_to_measure= list(range(n_qubits))\n","\n","    dev= qml.device(\"default.qubit\", wires=n_qubits)\n","\n","    @qml.qnode(dev)\n","    def density_circuit(params, ground_state):\n","        offset= 0\n","        qml.StatePrep(ground_state, wires=range(n_qubits))\n","        for _ in range(n_layers):\n","            for i in range(n_qubits):\n","                rx= params[offset+2*i]\n","                rz= params[offset+2*i+1]\n","                qml.RX(rx, wires=i)\n","                qml.RZ(rz, wires=i)\n","            offset+= 2*n_qubits\n","            for i in range(n_qubits):\n","                qml.CNOT(wires=[i,(i+1)%n_qubits])\n","\n","        return qml.density_matrix(wires=wires_to_measure)\n","\n","    return density_circuit\n","\n","##############################################################################\n","# 7) KERNEL: minimal changes => now they accept Nx2 for (J1,J2)\n","#    so inside the kernel, we build ground states on-the-fly\n","##############################################################################\n","\n","def build_kernel_full(n_qubits, n_layers, final_params):\n","    \"\"\"\n","    kernel_full(Xa, Xb):\n","       - Xa, Xb have shape (N,2), (M,2). each row => [J1, J2].\n","       - build ground state => pass to QNode => get full-wire density => overlap\n","    \"\"\"\n","    dens_qnode= create_density_qnode(n_qubits, n_layers=n_layers, wires_to_measure=None)\n","\n","    def kernel_fn(Xa, Xb):\n","        N= Xa.shape[0]\n","        M= Xb.shape[0]\n","        K= np.zeros((N,M))\n","\n","        for i in range(N):\n","            j1_i, j2_i= Xa[i,0], Xa[i,1]\n","            gs_i= ground_state_exact(n_qubits, j1_i, j2_i)\n","            rho_i= dens_qnode(final_params, gs_i)\n","\n","            for j in range(M):\n","                j1_j, j2_j= Xb[j,0], Xb[j,1]\n","                gs_j= ground_state_exact(n_qubits, j1_j, j2_j)\n","                rho_j= dens_qnode(final_params, gs_j)\n","                K[i,j]= np.real(np.trace(rho_i@rho_j))\n","\n","        return K\n","\n","    return kernel_fn\n","\n","def build_kernel_qubit(n_qubits, n_layers, final_params, qubit_index=[0]):\n","    \"\"\"\n","    single-qubit partial => measure only qubit_index => 2x2\n","    \"\"\"\n","    dens_qnode= create_density_qnode(n_qubits, n_layers=n_layers, wires_to_measure=qubit_index)\n","\n","    def kernel_fn(Xa, Xb):\n","        N= Xa.shape[0]\n","        M= Xb.shape[0]\n","        K= np.zeros((N,M))\n","\n","        for i in range(N):\n","            j1_i, j2_i= Xa[i,0], Xa[i,1]\n","            gs_i= ground_state_exact(n_qubits, j1_i, j2_i)\n","            rho_i= dens_qnode(final_params, gs_i)\n","\n","            for j in range(M):\n","                j1_j, j2_j= Xb[j,0], Xb[j,1]\n","                gs_j= ground_state_exact(n_qubits, j1_j, j2_j)\n","                rho_j= dens_qnode(final_params, gs_j)\n","                K[i,j]= np.real(np.trace(rho_i@rho_j))\n","\n","        return K\n","\n","    return kernel_fn\n","\n","##############################################################################\n","# 8) Minimal kernel ridge function => for demonstration\n","##############################################################################\n","\n","def kernel_ridge_mse(X, y, kernel_fn, alpha=1e-6):\n","    \"\"\"\n","    We do in-sample fit => y_pred => measure MSE\n","    X => Nx2, each row => (J1,J2)\n","    y => Nx\n","    kernel_fn => a function that does K(Xa, Xb)\n","    \"\"\"\n","    N= len(X)\n","    K= kernel_fn(X, X)\n","    # K += alpha*np.eye(N)\n","    alpha_vec= np.linalg.inv(K+alpha*np.eye(N))@ y\n","    y_pred= K@ alpha_vec\n","    mse= np.mean((y_pred- y)**2)\n","    return mse\n","\n","##############################################################################\n","# 9) The main training function + usage\n","##############################################################################\n","\n","def train_classifier_and_get_kernels(\n","    n_qubits=4,\n","    n_layers=2,\n","    Nx_train=6, Ny_train=6,\n","    Nx_test=10, Ny_test=10,\n","    j1_range=(-4,4), j2_range=(-4,4),\n","    max_steps=30,\n","    batch_size=4,\n","    lr=0.02\n","):\n","    \"\"\"\n","    1) build & train classifier with measure qubit0 => label=1 if above_l1 & below_l2\n","    2) build kernel_full, kernel_qubit for partial-wire\n","    returns (classifier_qnode, final_params, kernel_full, kernel_qubit0)\n","    \"\"\"\n","\n","    # A) create QNode, build data\n","    classifier_qnode, total_params= create_classifier_qnode(n_qubits, n_layers)\n","    states_train, labels_train, _, _= generate_data_grid(\n","        n_qubits, Nx_train, Ny_train,\n","        j1_range[0], j1_range[1], j2_range[0], j2_range[1],\n","        label_func=binary_label_function\n","    )\n","    states_test, labels_test, j1_test, j2_test= generate_data_grid(\n","        n_qubits, Nx_test, Ny_test,\n","        j1_range[0], j1_range[1], j2_range[0], j2_range[1],\n","        label_func=binary_label_function\n","    )\n","\n","    rng= np.random.default_rng(1)\n","    params= 0.1*rng.normal(size=(total_params,))\n","    opt= qml.AdamOptimizer(lr)\n","\n","    # mini-batch training\n","    N_train= len(states_train)\n","    all_indices= np.arange(N_train)\n","    num_batches= max(1, N_train//batch_size)\n","\n","    def cost_batch(p, idx_batch):\n","        bsize= len(idx_batch)\n","        sum_p= 0.0\n","        for i in idx_batch:\n","            lbl= labels_train[i]\n","            p2= classifier_qnode(p, states_train[i])\n","            sum_p+= p2[int(lbl)]\n","        return 1.0 - (sum_p/bsize)\n","\n","    for step in range(max_steps):\n","        rng.shuffle(all_indices)\n","        cost_sum= 0.0\n","        for b in range(num_batches):\n","            idx_b= all_indices[b*batch_size:(b+1)*batch_size]\n","            params, cval= opt.step_and_cost(lambda pp: cost_batch(pp, idx_b), params)\n","            cost_sum+= cval\n","        if step%5==0:\n","            ctrain= cost_fn(params, classifier_qnode, states_train, labels_train)\n","            acc_train= accuracy(params, classifier_qnode, states_train, labels_train)\n","            acc_test= accuracy(params, classifier_qnode, states_test, labels_test)\n","            print(f\"Step {step} => cost={ctrain:.4f}, TrainAcc={acc_train:.3f}, TestAcc={acc_test:.3f}\")\n","\n","    final_cost= cost_fn(params, classifier_qnode, states_train, labels_train)\n","    final_train_acc= accuracy(params, classifier_qnode, states_train, labels_train)\n","    final_test_acc= accuracy(params, classifier_qnode, states_test, labels_test)\n","    print(f\"Done => final cost={final_cost:.4f}, TrainAcc={final_train_acc:.3f}, TestAcc={final_test_acc:.3f}\")\n","\n","    # B) build kernel_full, kernel_qubit0\n","    kernel_full= build_kernel_full(n_qubits, n_layers, params)\n","    kernel_q0= build_kernel_qubit(n_qubits, n_layers, params, qubit_index=[0])\n","    kernel_q1= build_kernel_qubit(n_qubits, n_layers, params, qubit_index=[1])\n","    kernel_q2= build_kernel_qubit(n_qubits, n_layers, params, qubit_index=[2])\n","    kernel_q01= build_kernel_qubit(n_qubits, n_layers, params, qubit_index=[0,1])\n","    kernel_q12= build_kernel_qubit(n_qubits, n_layers, params, qubit_index=[1,2])\n","    kernel_q02= build_kernel_qubit(n_qubits, n_layers, params, qubit_index=[0,2])\n","    kernel_lst = [kernel_q0, kernel_q1, kernel_q2, kernel_q01, kernel_q12, kernel_q02]\n","\n","    return classifier_qnode, params, kernel_full, kernel_lst\n","\n","\n","def f_of_x(j1, j2, n_qubits, classifier_qnode, params):\n","    \"\"\"\n","    define y => circuit's probability of label=1\n","    ignoring actual label\n","    \"\"\"\n","    st= ground_state_exact(n_qubits, j1, j2)\n","    p2= classifier_qnode(params, st)\n","    return p2[1]\n","\n","def f_of_x01(J1, J2, n_qubits, classifier_qnode, params):\n","    \"\"\"\n","    define y => circuit's probability of label=1\n","    ignoring actual label\n","    \"\"\"\n","    below_l1= (J2 < -1 - J1)\n","    above_l1= (J2 > -1 - J1)\n","    above_l2= (J2 >  J1 - 1)\n","    below_l2= (J2 <  J1 - 1)\n","    # ignoring l3=1\n","\n","    if (above_l1 and below_l2):\n","        return 1\n","    else:\n","        return 0\n","\n","if __name__==\"__main__\":\n","    print(\"=== Minimal changes to use Nx2 for kernel inputs ===\")\n","    n_qubits=3 #4\n","    n_layers=2\n","\n","    # 1) train\n","    classifier_qnode, final_params, kernel_full, kernel_lst = train_classifier_and_get_kernels(\n","        n_qubits=n_qubits,\n","        n_layers=n_layers,\n","        Nx_train=6, Ny_train=6,\n","        Nx_test=10,  Ny_test=10,\n","        j1_range=(-4,4),\n","        j2_range=(-4,4),\n","        max_steps=0,\n","        batch_size=4,\n","        lr=0.02\n","    )\n","    kernel_q0, kernel_q1, kernel_q2, kernel_q01, kernel_q12, kernel_q02 = kernel_lst\n","\n","    # 2) build random dataset => Nx2 => (J1,J2)\n","    rng2= np.random.default_rng(123)\n","    Ndata= 20\n","    X_rand= np.zeros((Ndata,2), dtype=float)\n","    y_rand= np.zeros(Ndata, dtype=float)\n","    for i in range(Ndata):\n","        j1= rng2.uniform(-4,4)\n","        j2= rng2.uniform(-4,4)\n","        val= f_of_x01(j1, j2, n_qubits, classifier_qnode, final_params) + rng2.normal(0,0.1)\n","        X_rand[i,0]= j1\n","        X_rand[i,1]= j2\n","        y_rand[i]= val\n","\n","    # 3) apply kernel_full => kernel ridge => MSE\n","    # mse_full= kernel_ridge_mse(X_rand, y_rand, kernel_full, alpha=1e-6)\n","    # print(f\"MSE(full-wire) = {mse_full:.5f}\")\n","\n","    # # 4) partial-wire qubit0 => MSE\n","    # mse_q0= kernel_ridge_mse(X_rand, y_rand, kernel_q0, alpha=1e-6)\n","    # print(f\"MSE(part-wire[0]) = {mse_q0:.5f}\")\n","\n","    # # 4) partial-wire qubit0 => MSE\n","    # mse_q0= kernel_ridge_mse(X_rand, y_rand, kernel_q2, alpha=1e-6)\n","    # print(f\"MSE(part-wire[2]) = {mse_q0:.5f}\")\n","\n","    # # 4) partial-wire qubit0 => MSE\n","    # mse_q0= kernel_ridge_mse(X_rand, y_rand, kernel_q01, alpha=1e-6)\n","    # print(f\"MSE(part-wire[0,1]) = {mse_q0:.5f}\")\n","\n","    for k in kernel_lst:\n","      print(kernel_ridge_mse(X_rand, y_rand, k, alpha=1e-6))\n","    print(kernel_ridge_mse(X_rand, y_rand, kernel_full, alpha=1e-6))\n"]},{"cell_type":"markdown","metadata":{"id":"TxMYMBb1SuU9"},"source":["# P greedy code"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MSumxY0FrBbd"},"outputs":[],"source":["import numpy as np\n","import math\n","\n","\n","\n","\n","##############################################################################\n","# 3. P-greedy ALGORITHM with fixed max_m dimension\n","##############################################################################\n","def p_greedy_newton_basis_maxM(kernel_func, Omega_b, max_m):\n","    \"\"\"\n","    Runs P-greedy up to 'max_m' points, returning:\n","      - X_indices: list of chosen center indices\n","      - Nvals    : shape (max_m, nB) array\n","                   Nvals[m_idx, i] = N_{m_idx+1}(Omega_b[i]).\n","      - P2       : final array of P^2_{max_m}(x_i)\n","    \"\"\"\n","    nB = len(Omega_b)\n","    if nB == 0:\n","        return [], np.zeros((max_m, 0)), []\n","\n","    # Precompute diagonal: K(x_i, x_i)\n","    diagK = np.array([kernel_func(Omega_b[i], Omega_b[i]) for i in range(nB)])\n","    Nvals = np.zeros((max_m, nB))\n","\n","    # 1) pick first center\n","    i1 = np.argmax(diagK)\n","    X_indices = [i1]\n","\n","    denom_first = math.sqrt(diagK[i1]) if diagK[i1] > 1e-14 else 1e-14\n","    for i in range(nB):\n","        Nvals[0, i] = kernel_func(Omega_b[i], Omega_b[i1]) / denom_first\n","\n","    # Initialize power-function squared\n","    P2 = diagK - Nvals[0,:]**2\n","\n","    m = 1\n","    while m < max_m:\n","        # pick next center\n","        i_next = np.argmax(P2)\n","        X_indices.append(i_next)\n","\n","        denom = math.sqrt(P2[i_next]) if P2[i_next] > 1e-14 else 1e-14\n","\n","        # Build next Newton basis\n","        for i in range(nB):\n","            val = kernel_func(Omega_b[i], Omega_b[i_next])\n","            tmp = 0.0\n","            for k in range(m):\n","                tmp += Nvals[k, i_next]*Nvals[k, i]\n","            val -= tmp\n","            Nvals[m, i] = val / denom\n","\n","        # update P^2\n","        P2 = P2 - Nvals[m,:]**2\n","        m += 1\n","\n","    return X_indices, Nvals, P2\n","\n","\n","##############################################################################\n","# 4. Build an approximate-kernel function from the P-greedy basis\n","##############################################################################\n","def build_approx_kernel(Omega_b, Nvals, m):\n","    \"\"\"\n","    Returns a function approx_kernel(x, y) that:\n","      approx_kernel(x_i, x_j) = sum_{k=0..m-1} Nvals[k, i]*Nvals[k, j]\n","    provided x_i, x_j are exactly in Omega_b.\n","    \"\"\"\n","    nB = len(Omega_b)\n","\n","    # Map each mesh point -> index\n","    # (We store them as tuples for dictionary keys.)\n","    index_map = {}\n","    for i in range(nB):\n","        # caution: floating comparisons. For many bandit tasks this is okay if\n","        # we always pass EXACT points from Omega_b. If new points appear, you'll need\n","        # extra logic to evaluate Nvals on them.\n","        index_map[tuple(Omega_b[i])] = i\n","\n","    def compute_approx_kernel(x, y):\n","        \"\"\"\n","        Return the approximate kernel for x,y in Omega_b\n","        \"\"\"\n","        i = index_map[tuple(x)]\n","        j = index_map[tuple(y)]\n","        val = 0.0\n","        for k in range(m):\n","            val += Nvals[k, i]*Nvals[k, j]\n","        return val\n","\n","    Kernel = np.zeros((nB, nB))\n","    for i in Omega_b:\n","        for j in Omega_b:\n","            Kernel[index_map[tuple(i)], index_map[tuple(j)]] = compute_approx_kernel(i, j)\n","\n","    def approx_kernel(x,y):\n","      return Kernel[index_map[tuple(x)], index_map[tuple(y)]]\n","\n","\n","    return approx_kernel\n","\n","\n","##############################################################################\n","# 5. The function you requested: get_approximate_kernel_using_P_greedy(dim)\n","##############################################################################\n","def get_approximate_kernel_using_P_greedy(dim):\n","    \"\"\"\n","    1) Construct a 3D mesh (10x10x10) as candidate set Omega_b.\n","    2) Run p_greedy_newton_basis_maxM(..., max_m=dim).\n","    3) Build and return approx_kernel(x,y) for x,y in Omega_b.\n","    \"\"\"\n","    # This is your discrete domain for P-greedy:\n","    Omega_b = build_grid([(-4,4),(-4,4)],20)\n","\n","    # Run P-greedy up to 'dim' basis functions using your \"full_kernel_classic\"\n","    X_indices, Nvals, _ = p_greedy_newton_basis_maxM(full_kernel_classic, Omega_b, max_m=dim)\n","\n","    # Build the approximate-kernel function\n","    approx_kernel_func = build_approx_kernel(Omega_b, Nvals, dim)\n","\n","    # Return the callable that you can use inside 'kernell(X, Y)'\n","    return approx_kernel_func\n","\n","\n","##############################################################################\n","# 6. Demo usage (Optional)\n","##############################################################################\n","if __name__ == \"__main__\":\n","    full_kernel_classic = lambda x,y: kernel_full(np.array([x]),np.array([y]))[0][0]\n","    # Example: get an approximate kernel with dim=5\n","    approxK = get_approximate_kernel_using_P_greedy(11)\n","\n","    # Compare the approximate kernel vs. the true kernel on a random pair:\n","    Omega_test = build_grid([(-4,4),(-4,4)],20)\n","    idxA, idxB = 53, 56\n","    xA, xB = Omega_test[idxA], Omega_test[idxB]\n","\n","    true_val = full_kernel_classic(xA, xB)\n","    approx_val = approxK(xA, xB)\n","    print(f\"True K: {true_val:.6f}, Approx K: {approx_val:.6f}\")\n"]},{"cell_type":"markdown","metadata":{"id":"Y5xVKn03bu0G"},"source":["# whole domain"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5LiIqh4qjnyH"},"outputs":[],"source":["def f_of_x01(J1, J2, n_qubits, classifier_qnode, params):\n","    \"\"\"\n","    define y => circuit's probability of label=1\n","    ignoring actual label\n","    \"\"\"\n","    below_l1= (J2 < -1 - J1)\n","    above_l1= (J2 > -1 - J1)\n","    above_l2= (J2 >  J1 - 1)\n","    below_l2= (J2 <  J1 - 1)\n","    # ignoring l3=1\n","\n","    if (above_l1 and below_l2):\n","        return 1\n","    else:\n","        return 0\n","\n","\n","\n","w_random = np.array([1 for _ in range(27)])\n","# setup\n","qubits = 3\n","seed = 1\n","bounds = [(-4,4),(-4,4)]\n","\n","# get kernels and true reward functions\n","\n","dim_tried = []\n","regret_for_dim = []\n","std_regret_for_dim = []\n","\n","quantum_f = lambda X: np.array([f_of_x01(X[i][0], X[i][1], 3, classifier_qnode, final_params) for i in range(X.shape[0])])\n","\n","for dim in trange(1,11,1, desc=\"Number of projected kernels Used\"):\n","\n","  approx_kernel = get_approximate_kernel_using_P_greedy(dim)\n","\n","  def kernell(X,Y):\n","    K = np.zeros((X.shape[0],Y.shape[0]))\n","    for i in range(X.shape[0]):\n","      for j in range(Y.shape[0]):\n","        K[i,j] = approx_kernel(X[i],Y[j])\n","    return K\n","\n","\n","  gp = GaussianProcessRegressor(kernel=kernell, alpha=1e-3)\n","  avg_best, best_hist_all,std_regret = run_multiple_experiments_gp_ucb(\n","  f=quantum_f,\n","  gp=gp,\n","  bounds=bounds,\n","  n_runs=30,\n","  n_iter=100,\n","  init_points=1,\n","  n_grid=20,\n","  beta_func=None,  # use the default inside gp_ucb_nd\n","  random_state=42,\n","  verbose=False,\n","  error = 1/(dim)\n",")\n","\n","\n","  dim_tried.append(dim)\n","  regret_for_dim.append(avg_best[-1])\n","  std_regret_for_dim.append(std_regret[-1])\n","  print(avg_best)\n","\n","\n","\n","\n","plt.errorbar(dim_tried,regret_for_dim,std_regret_for_dim,ls=\"-\",\n","             marker='d',\n","             color=\"#009E73\",\n","             alpha=1.0,\n","             capsize=4)\n","plt.title(\"Regret for Different Kernel Used\")\n","plt.xlabel(\"Number of Projected Kernel Used for Modeling\")\n","plt.ylabel(\"Regret for T=100\")\n","plt.grid(True)\n","plt.legend()\n","plt.show()\n","\n","print(regret_for_dim)\n","print(std_regret_for_dim)\n"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}