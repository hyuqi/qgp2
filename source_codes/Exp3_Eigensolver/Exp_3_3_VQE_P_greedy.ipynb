{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"kexeEKmZsjlk"},"outputs":[],"source":["pip install pennylane"]},{"cell_type":"markdown","metadata":{"id":"Xe15VI0eXqSk"},"source":["# GP"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QrfrrgdV0s4C"},"outputs":[],"source":["################################################################################\n","# Quantum-Kernel-based BO for a 3-Qubit PennyLane Circuit Minimizing Ising Energy\n","################################################################################\n","\n","import numpy as np\n","import math\n","import matplotlib.pyplot as plt\n","from functools import lru_cache\n","from scipy.special import erf\n","\n","import pennylane as qml\n","from pennylane import numpy as pnp\n","\n","##############################################################################\n","# 1) The Provided GaussianProcessRegressor Class (from your prompt)\n","##############################################################################\n","class GaussianProcessRegressor:\n","    \"\"\"\n","    Gaussian Process Regressor for d-dimensional inputs.\n","    Returns both predictive mean and std if requested.\n","    \"\"\"\n","    def __init__(self, kernel, alpha=1e-5):\n","        self.kernel = kernel\n","        self.alpha = alpha\n","        self.X_train = None\n","        self.y_train = None\n","        self.K_inv = None\n","\n","    def fit(self, X, y):\n","        X = np.array(X)\n","        y = np.array(y)\n","        self.X_train = X\n","        self.y_train = y\n","        K = self.kernel(X, X)\n","        K += self.alpha * np.eye(len(X))\n","        self.K_inv = np.linalg.inv(K)\n","\n","    def predict(self, X_test, return_std=False):\n","        X_test = np.array(X_test)\n","        K_star = self.kernel(X_test, self.X_train)\n","        y_mean = K_star @ (self.K_inv @ self.y_train)\n","\n","        if return_std:\n","            K_star_star = self.kernel(X_test, X_test)\n","            cov = K_star_star - K_star @ self.K_inv @ K_star.T\n","            var = np.diag(cov)\n","            var = np.maximum(var, 0.0)\n","            y_std = np.sqrt(var)\n","            return y_mean, y_std\n","        else:\n","            return y_mean\n","\n","##############################################################################\n","# 2) Define the 3-qubit circuit & Hamiltonian for Ising-like model\n","##############################################################################\n","num_qubits = 3\n","num_layers = 5\n","dev = qml.device(\"default.qubit\", wires=num_qubits, shots=None)\n","\n","# We'll define an Ising Hamiltonian with effectively + X_jX_{j+1} + Z_j\n","# by setting Jx=-1, hz=-1 in the \"paper's sign\" => sum_j X_jX_j+1 + sum_j Z_j\n","# We'll just build a PennyLane Hamiltonian similarly:\n","obs = []\n","coeffs = []\n","\n","# Coupling: X_j X_{j+1}\n","for j in range(num_qubits - 1):\n","    obs.append(qml.PauliX(j) @ qml.PauliX(j+1))\n","    coeffs.append(1.0)  # effectively +1\n","\n","# Local Z\n","for j in range(num_qubits):\n","    obs.append(qml.PauliZ(j))\n","    coeffs.append(1.0)\n","\n","H = qml.Hamiltonian(coeffs, obs)\n","\n","# dimension => (2 + 2*num_layers)*num_qubits => (2+2*3)*3 => 24\n","D = (2 + 2*num_layers)*num_qubits\n","\n","@qml.qnode(dev)\n","def circuit_energy(params_flat):\n","    \"\"\"\n","    PennyLane QNode that prepares the 3-qubit, 3-layer circuit (24 params),\n","    then measures the expectation of H.\n","    \"\"\"\n","    params_reshaped = params_flat.reshape((2*(num_layers+1), num_qubits))  # shape (8,3)\n","    # 1) initial block\n","    for q in range(num_qubits):\n","        qml.RY(params_reshaped[0,q], wires=q)\n","        qml.RZ(params_reshaped[1,q], wires=q)\n","\n","    # 2) repeated layers\n","    idx=2\n","    for _layer in range(num_layers):\n","        # entangle\n","        qml.CNOT(wires=[0,1])\n","        qml.CNOT(wires=[0,2])\n","        qml.CNOT(wires=[1,2])\n","        # single-qubit\n","        for q in range(num_qubits):\n","            qml.RY(params_reshaped[idx,q], wires=q)\n","            qml.RZ(params_reshaped[idx+1,q], wires=q)\n","        idx+=2\n","\n","    return qml.expval(H)\n","\n","def cost_fn(params_flat):\n","    return float(circuit_energy(params_flat))\n","\n","##############################################################################\n","# 3) Build a \"quantum fidelity kernel\" => overlap of states\n","##############################################################################\n","# We'll define a QNode that returns state, then compute fidelity = |<psi(x)|psi(y)>|^2\n","# For efficiency, store states in a cache so we don't recalc them repeatedly.\n","\n","@qml.qnode(dev)\n","def circuit_state(params_flat):\n","    \"\"\"\n","    Returns the state vector (1D complex array) from the same circuit,\n","    so we can compute fidelity.\n","    \"\"\"\n","    params_reshaped = params_flat.reshape((2*(num_layers+1), num_qubits))\n","    # same structure as above\n","    for q in range(num_qubits):\n","        qml.RY(params_reshaped[0,q], wires=q)\n","        qml.RZ(params_reshaped[1,q], wires=q)\n","    idx=2\n","    for _layer in range(num_layers):\n","        qml.CNOT(wires=[0,1])\n","        qml.CNOT(wires=[0,2])\n","        qml.CNOT(wires=[1,2])\n","        for q in range(num_qubits):\n","            qml.RY(params_reshaped[idx,q], wires=q)\n","            qml.RZ(params_reshaped[idx+1,q], wires=q)\n","        idx+=2\n","    return qml.state()\n","\n","# We'll store states in an LRU cache for fast repeated calls\n","@lru_cache(None)\n","def get_state(params_tuple):\n","    \"\"\"\n","    params_tuple is a tuple of 24 floats. We'll convert to a pnp.array and run circuit_state.\n","    We store the result in a cache for speed.\n","    \"\"\"\n","    arr = pnp.array(params_tuple, dtype=pnp.float64)\n","    return circuit_state(arr)\n","\n","def quantum_fidelity_kernel(X, Y):\n","    \"\"\"\n","    For each pair (x_i, y_j) in X, Y, compute fidelity = |<psi(x_i)|psi(y_j)>|^2\n","    X => NxD, Y => MxD\n","    We'll do NxM result.\n","    We'll store states in a dictionary to avoid repeated circuit calls.\n","    \"\"\"\n","    X = np.atleast_2d(X)\n","    Y = np.atleast_2d(Y)\n","    N, D_ = X.shape\n","    M, D2_ = Y.shape\n","    assert D_==D2_, \"Dimension mismatch\"\n","    K = np.zeros((N,M), dtype=float)\n","    for i in range(N):\n","        # convert X[i] to tuple\n","        xi_tuple = tuple(X[i])\n","        psi_i = get_state(xi_tuple)  # 2^3=8 dim complex array\n","        # we only do conj once\n","        conj_psi_i = np.conjugate(psi_i)\n","        for j in range(M):\n","            yj_tuple = tuple(Y[j])\n","            psi_j = get_state(yj_tuple)\n","            overlap = np.vdot(psi_i, psi_j)  # or conj_psi_i @ psi_j\n","            # fidelity\n","            val = abs(overlap)**2\n","            K[i,j] = val\n","    return K\n","\n","##############################################################################\n","# 4) Simple BO approach with this quantum kernel GP\n","##############################################################################\n","def expected_improvement(X_star, f_best, gp):\n","    \"\"\"\n","    Standard EI => for each x in X_star, compute:\n","      mu, std => z=(f_best - mu)/std\n","      pdf, cdf => improvement = (f_best - mu)*cdf + std*pdf\n","    We pick the argmax.\n","    \"\"\"\n","    mu, std = gp.predict(X_star, return_std=True)\n","    ei = np.zeros_like(mu)\n","    mask = std>1e-12\n","    z = (f_best - mu[mask]) / std[mask]\n","    pdf = 1./np.sqrt(2.*math.pi)*np.exp(-0.5*z**2)\n","    cdf = 0.5*(1.+erf(z/np.sqrt(2.)))\n","    improvement = (f_best - mu[mask])*cdf + std[mask]*pdf\n","    improvement = np.maximum(improvement, 0.)\n","    ei[mask] = improvement\n","\n","    ei = -(mu - 2*std)\n","    return ei\n","\n","def quantum_kernel_bo(n_init=5, n_iter=20, seed=0, n_cand=1000):\n","    np.random.seed(seed)\n","    # dimension is 24\n","    # 1) gather initial data\n","    X_data = np.random.rand(n_init, D)*2.*math.pi\n","    y_data = np.array([cost_fn(row) for row in X_data])\n","\n","    # 2) build GP with quantum_fidelity_kernel\n","    gp = GaussianProcessRegressor(kernel=quantum_fidelity_kernel, alpha=1e-9)\n","    gp.fit(X_data, y_data)\n","\n","    best_history = [y_data.min()]\n","\n","    for step in range(n_iter):\n","        # 3) find next candidate x via EI over random set\n","        f_best = y_data.min()\n","        X_cand = np.random.rand(n_cand, D)*2.*math.pi\n","        ei_vals = expected_improvement(X_cand, f_best, gp)\n","        idx_best = np.argmax(ei_vals)\n","        x_next = X_cand[idx_best]\n","        y_next = cost_fn(x_next)\n","        # 4) update training data\n","        X_data = np.vstack([X_data, x_next])\n","        y_data = np.concatenate([y_data, [y_next]])\n","        gp.fit(X_data, y_data)\n","        best_so_far = min(best_history[-1], y_next)\n","        best_history.append(best_so_far)\n","\n","        print(f\"Iteration {step+1:2d}, best so far = {best_so_far:.6f}\")\n","\n","    return X_data, y_data, best_history\n","\n","##############################################################################\n","# 5) Run it\n","##############################################################################\n","if __name__==\"__main__\":\n","    X_data, y_data, best_hist = quantum_kernel_bo(n_init=5, n_iter=100, seed=None, n_cand=100)\n","    print(\"Final best energy found:\", best_hist[-1])\n","    plt.plot(best_hist, label=\"Best energy so far\")\n","    plt.xlabel(\"Iteration\")\n","    plt.ylabel(\"Energy\")\n","    plt.title(\"BO with Quantum Fidelity Kernel (3-Qubit Ising, 24 params)\")\n","    plt.legend()\n","    plt.show()\n"]},{"cell_type":"markdown","metadata":{"id":"YyOY0c3dmn0L"},"source":["## projected kernels"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fsTBJbX2acgN"},"outputs":[],"source":["##############################################################################\n","# 1) Imports & Setup\n","##############################################################################\n","import numpy as np\n","import math\n","import matplotlib.pyplot as plt\n","from functools import lru_cache\n","from scipy.special import erf\n","\n","import pennylane as qml\n","from pennylane import numpy as pnp\n","\n","##############################################################################\n","# 2) GaussianProcessRegressor (unchanged from your prompt)\n","##############################################################################\n","class GaussianProcessRegressor:\n","    \"\"\"\n","    Gaussian Process Regressor for d-dimensional inputs.\n","    Returns both predictive mean and std if requested.\n","    \"\"\"\n","    def __init__(self, kernel, alpha=1e-5):\n","        self.kernel = kernel\n","        self.alpha = alpha\n","        self.X_train = None\n","        self.y_train = None\n","        self.K_inv = None\n","\n","    def fit(self, X, y):\n","        X = np.array(X)\n","        y = np.array(y)\n","        self.X_train = X\n","        self.y_train = y\n","        K = self.kernel(X, X)\n","        K += self.alpha * np.eye(len(X))\n","        self.K_inv = np.linalg.inv(K)\n","\n","    def predict(self, X_test, return_std=False):\n","        X_test = np.array(X_test)\n","        K_star = self.kernel(X_test, self.X_train)\n","        y_mean = K_star @ (self.K_inv @ self.y_train)\n","\n","        if return_std:\n","            K_star_star = self.kernel(X_test, X_test)\n","            cov = K_star_star - K_star @ self.K_inv @ K_star.T\n","            var = np.diag(cov)\n","            var = np.maximum(var, 0.0)\n","            y_std = np.sqrt(var)\n","            return y_mean, y_std\n","        else:\n","            return y_mean\n","\n","##############################################################################\n","# 3) Setup: 3-qubit circuit, Ising Hamiltonian, cost function\n","##############################################################################\n","num_qubits = 5\n","num_layers = 1\n","dev = qml.device(\"default.qubit\", wires=num_qubits, shots=None)\n","\n","# Build H = sum_j X_jX_{j+1} + sum_j Z_j\n","obs = []\n","coeffs = []\n","for j in range(num_qubits-1):\n","    obs.append(qml.PauliX(j) @ qml.PauliX(j+1))\n","    coeffs.append(1.0)\n","for j in range(num_qubits):\n","    obs.append(qml.PauliZ(j))\n","    coeffs.append(1.0)\n","H = qml.Hamiltonian(coeffs, obs)\n","\n","# dimension => (2 + 2*num_layers)*num_qubits => 24 if num_layers=3,\n","# but here we set num_layers=1 for a simpler example => (2+2*1)*3=6 parameters\n","D = (2 + 2*num_layers)*num_qubits\n","\n","@qml.qnode(dev)\n","def circuit_energy(params_flat):\n","    \"\"\"\n","    Prepare the circuit (with 'num_layers'=1) and measure expectation of H.\n","    \"\"\"\n","    params_reshaped = params_flat.reshape((2*(num_layers+1), num_qubits))\n","    # 1) initial block\n","    for q in range(num_qubits):\n","        qml.RY(params_reshaped[0,q], wires=q)\n","        qml.RZ(params_reshaped[1,q], wires=q)\n","    idx=2\n","    # repeated layers\n","    for _layer in range(num_layers):\n","        qml.CNOT(wires=[0,1])\n","        qml.CNOT(wires=[0,2])\n","        qml.CNOT(wires=[1,2])\n","        for q in range(num_qubits):\n","            qml.RY(params_reshaped[idx,q], wires=q)\n","            qml.RZ(params_reshaped[idx+1,q], wires=q)\n","        idx+=2\n","    return qml.expval(H)\n","\n","def cost_fn(params_flat):\n","    return float(circuit_energy(params_flat))\n","\n","##############################################################################\n","# 4) Partial-Density QNode => measure subset of qubits' density matrix\n","##############################################################################\n","# We'll specify `subset_wires` as a list of qubit indices. Then we do\n","# qml.density_matrix(subset_wires). This returns a 2^(|subset|) x 2^(|subset|) complex matrix,\n","# which might be mixed if the global state is partially traced out.\n","\n","@qml.qnode(dev)\n","def circuit_partial_density(params_flat, subset_wires):\n","    \"\"\"\n","    Returns the partial density matrix for a specified subset of qubits.\n","    The rest are traced out automatically by PennyLane.\n","    \"\"\"\n","    # same circuit as above\n","    params_reshaped = params_flat.reshape((2*(num_layers+1), num_qubits))\n","    # initial\n","    for q in range(num_qubits):\n","        qml.RY(params_reshaped[0,q], wires=q)\n","        qml.RZ(params_reshaped[1,q], wires=q)\n","    idx=2\n","    for _layer in range(num_layers):\n","        qml.CNOT(wires=[0,1])\n","        qml.CNOT(wires=[0,2])\n","        qml.CNOT(wires=[1,2])\n","        for q in range(num_qubits):\n","            qml.RY(params_reshaped[idx,q], wires=q)\n","            qml.RZ(params_reshaped[idx+1,q], wires=q)\n","        idx+=2\n","\n","    return qml.density_matrix(wires=subset_wires)\n","\n","@lru_cache(None)\n","def get_partial_density(params_tuple, subset_wires_tuple):\n","    \"\"\"\n","    Cache for partial density matrix.\n","    PennyLane expects a python list for `wires=...`,\n","    but we can't store a list as a cache key, so we use a tuple of int.\n","    \"\"\"\n","    arr = pnp.array(params_tuple, dtype=pnp.float64)\n","    # convert subset_wires_tuple back to a list\n","    wires_list = list(subset_wires_tuple)\n","    dm = circuit_partial_density(arr, wires_list)\n","    return dm\n","\n","def partial_density_kernel(X, Y, subset_wires=[0,1], method=\"hilbert_schmidt\"):\n","    \"\"\"\n","    Projected kernel on a subset of qubits.\n","    For each x_i,y_j, we get partial density matrices => dm_x, dm_y.\n","    Then define kernel = Tr(dm_x dm_y) if method=\"hilbert_schmidt\"\n","    (One can define other overlaps, e.g. Uhlmann fidelity, but we do simpler HS.)\n","    \"\"\"\n","    X = np.atleast_2d(X)\n","    Y = np.atleast_2d(Y)\n","    N, dx = X.shape\n","    M, dx2 = Y.shape\n","    assert dx==dx2, \"Dimension mismatch\"\n","\n","    # Turn subset_wires into a tuple so it can be used in a cache key\n","    wires_tuple = tuple(subset_wires)\n","    K = np.zeros((N,M), dtype=float)\n","    for i in range(N):\n","        x_tuple = tuple(X[i])\n","        dm_x = get_partial_density(x_tuple, wires_tuple)\n","        for j in range(M):\n","            y_tuple = tuple(Y[j])\n","            dm_y = get_partial_density(y_tuple, wires_tuple)\n","            if method==\"hilbert_schmidt\":\n","                # HS overlap = Tr(dm_x dm_y)\n","                # dm_x, dm_y are pnp arrays => convert to np\n","                val = np.real(np.trace(dm_x @ dm_y))\n","                K[i,j]=val\n","            else:\n","                raise ValueError(\"Only 'hilbert_schmidt' method is implemented.\")\n","    return K\n","\n","##############################################################################\n","# 5) Basic Bayesian Optimization with random candidate sampling + EI\n","##############################################################################\n","def expected_improvement(X_star, f_best, gp):\n","    mu, std = gp.predict(X_star, return_std=True)\n","    ei = np.zeros_like(mu)\n","    mask = std>1e-12\n","    z = (f_best - mu[mask]) / std[mask]\n","    pdf = 1./np.sqrt(2.*math.pi)*np.exp(-0.5*z**2)\n","    cdf = 0.5*(1.+erf(z/np.sqrt(2.)))\n","    improvement = (f_best - mu[mask])*cdf + std[mask]*pdf\n","    improvement = np.maximum(improvement, 0.)\n","    ei[mask]=improvement\n","    return ei\n","\n","def bo_with_partial_kernel(\n","    n_init=5,\n","    n_iter=20,\n","    n_cand=100,\n","    seed=0,\n","    dim = 7\n","):\n","    np.random.seed(seed)\n","    # init data\n","    X_data = np.random.rand(n_init, D)*2.*math.pi\n","    y_data = np.array([cost_fn(x) for x in X_data])\n","    # build kernel\n","    def kernel_fn1(X, Y):\n","        return partial_density_kernel(X, Y, subset_wires=[0], method=\"hilbert_schmidt\")\n","    def kernel_fn2(X, Y):\n","        return partial_density_kernel(X, Y, subset_wires=[1], method=\"hilbert_schmidt\")\n","    def kernel_fn3(X, Y):\n","        return partial_density_kernel(X, Y, subset_wires=[2], method=\"hilbert_schmidt\")\n","    def kernel_fn4(X, Y):\n","        return partial_density_kernel(X, Y, subset_wires=[0,2], method=\"hilbert_schmidt\")\n","    def kernel_fn5(X, Y):\n","        return partial_density_kernel(X, Y, subset_wires=[1,2], method=\"hilbert_schmidt\")\n","    def kernel_fn6(X, Y):\n","        return partial_density_kernel(X, Y, subset_wires=[0,1], method=\"hilbert_schmidt\")\n","    def kernel_fn7(X, Y):\n","        return partial_density_kernel(X, Y, subset_wires=[0,1,2], method=\"hilbert_schmidt\")\n","\n","    kernels = [kernel_fn1,kernel_fn2,kernel_fn3,kernel_fn4,kernel_fn5,kernel_fn6]\n","\n","    def kernel_fn(X, Y):\n","        print(dim)\n","        if dim == 7:\n","          return kernel_fn7(X, Y)\n","        else:\n","          res = 0\n","          for i in range(dim):\n","            res += kernels[i](X,Y)\n","          return res\n","\n","    gp = GaussianProcessRegressor(kernel=kernel_fn, alpha=1e-8)\n","    gp.fit(X_data, y_data)\n","\n","    best_hist=[y_data.min()]\n","    for step in range(n_iter):\n","        f_best = y_data.min()\n","        X_cand = np.random.rand(n_cand, D)*2.*math.pi\n","        ei_vals=expected_improvement(X_cand, f_best, gp)\n","        idx=np.argmax(ei_vals)\n","        x_next=X_cand[idx]\n","        y_next=cost_fn(x_next)\n","\n","        X_data = np.vstack([X_data, x_next])\n","        y_data = np.concatenate([y_data, [y_next]])\n","        gp.fit(X_data, y_data)\n","        best_so_far=min(best_hist[-1], y_next)\n","        best_hist.append(best_so_far)\n","        #print(f\"Iteration {step+1:2d}, best so far = {best_so_far:.6f}\")\n","    return X_data, y_data, best_hist\n","\n","##############################################################################\n","# 6) Run\n","##############################################################################\n","# if __name__==\"__main__\":\n","#     # Example usage: define partial kernel on qubits [0,2]\n","#     # Then do BO\n","\n","#     trials = 3\n","#     energy_final = []\n","#     energy_final_std = []\n","\n","#     for dim in range(1,8):\n","#         energies = []\n","#         for trial in range(trials):\n","#             X_data, y_data, best_hist = bo_with_partial_kernel(\n","#                 n_init=3,\n","#                 n_iter=100,\n","#                 n_cand=100,\n","#                 seed=None,\n","#                 dim = dim\n","#             )\n","#             energies.append(best_hist)\n","#             print(f\"dim {dim} completed trial {trial}\")\n","#             print(\"Final best energy found:\", best_hist[-1])\n","#         energy_all = np.array(energies)\n","#         avg_energy = energy_all.mean(axis=0)\n","#         std_energy = energy_all.std(axis=0)\n","\n","#         energy_final.append(avg_energy[-1])\n","#         energy_final_std.append(std_energy[-1])\n","\n","\n","\n","#     plt.errorbar(np.array([1,2,3,4,5,6,7]), np.array(energy_final), np.array(energy_final_std), ls=\"-\",\n","#                 marker='d',\n","#                 color=\"#009E73\",\n","#                 alpha=1.0,\n","#                 capsize=4)\n"]},{"cell_type":"markdown","metadata":{"id":"4ql30DbvXlHF"},"source":["# GP with LFGBS"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XDVELGea06H1"},"outputs":[],"source":["##############################################################################\n","# Imports\n","##############################################################################\n","import numpy as np\n","import math\n","import matplotlib.pyplot as plt\n","from functools import lru_cache\n","from scipy.optimize import minimize\n","from scipy.special import erf\n","\n","import pennylane as qml\n","from pennylane import numpy as pnp\n","\n","##############################################################################\n","# 1) The same GaussianProcessRegressor class from your prompt\n","##############################################################################\n","class GaussianProcessRegressor:\n","    \"\"\"\n","    Gaussian Process Regressor for d-dimensional inputs.\n","    Returns both predictive mean and std if requested.\n","    \"\"\"\n","    def __init__(self, kernel, alpha=1e-5):\n","        self.kernel = kernel\n","        self.alpha = alpha\n","        self.X_train = None\n","        self.y_train = None\n","        self.K_inv = None\n","\n","    def fit(self, X, y):\n","        X = np.array(X)\n","        y = np.array(y)\n","        self.X_train = X\n","        self.y_train = y\n","        K = self.kernel(X, X)\n","        K += self.alpha * np.eye(len(X))\n","        self.K_inv = np.linalg.inv(K)\n","\n","    def predict(self, X_test, return_std=False):\n","        X_test = np.array(X_test)\n","        K_star = self.kernel(X_test, self.X_train)\n","        y_mean = K_star @ (self.K_inv @ self.y_train)\n","\n","        if return_std:\n","            K_star_star = self.kernel(X_test, X_test)\n","            cov = K_star_star - K_star @ self.K_inv @ K_star.T\n","            var = np.diag(cov)\n","            var = np.maximum(var, 0.0)\n","            y_std = np.sqrt(var)\n","            return y_mean, y_std\n","        else:\n","            return y_mean\n","\n","##############################################################################\n","# 2) Define 3-qubit circuit (24 params) + cost function\n","##############################################################################\n","num_qubits = 3\n","num_layers = 1\n","D = (2 + 2*num_layers)*num_qubits  # 24\n","dev = qml.device(\"default.qubit\", wires=num_qubits, shots=None)\n","\n","obs = []\n","coeffs = []\n","# Coupling X_j X_{j+1}\n","for j in range(num_qubits - 1):\n","    obs.append(qml.PauliX(j) @ qml.PauliX(j+1))\n","    coeffs.append(1.0)\n","# Local Z\n","for j in range(num_qubits):\n","    obs.append(qml.PauliZ(j))\n","    coeffs.append(1.0)\n","\n","H = qml.Hamiltonian(coeffs, obs)\n","\n","@qml.qnode(dev)\n","def circuit_energy(params):\n","    params_reshaped = params.reshape((2*(num_layers+1), num_qubits))\n","    # initial block\n","    for q in range(num_qubits):\n","        qml.RY(params_reshaped[0,q], wires=q)\n","        qml.RZ(params_reshaped[1,q], wires=q)\n","    idx=2\n","    for _layer in range(num_layers):\n","        qml.CNOT(wires=[0,1])\n","        qml.CNOT(wires=[0,2])\n","        qml.CNOT(wires=[1,2])\n","        for q in range(num_qubits):\n","            qml.RY(params_reshaped[idx,q], wires=q)\n","            qml.RZ(params_reshaped[idx+1,q], wires=q)\n","        idx+=2\n","    return qml.expval(H)\n","\n","def cost_fn(params):\n","    return float(circuit_energy(params))\n","\n","##############################################################################\n","# 3) Quantum Fidelity Kernel (Same as previous \"fidelity\" approach)\n","##############################################################################\n","@qml.qnode(dev)\n","def circuit_state(params):\n","    \"\"\"\n","    Return state vector from same circuit. We'll cache it for fidelity calcs.\n","    \"\"\"\n","    params_reshaped = params.reshape((2*(num_layers+1), num_qubits))\n","    # same arrangement\n","    for q in range(num_qubits):\n","        qml.RY(params_reshaped[0,q], wires=q)\n","        qml.RZ(params_reshaped[1,q], wires=q)\n","    idx=2\n","    for _layer in range(num_layers):\n","        qml.CNOT(wires=[0,1])\n","        qml.CNOT(wires=[0,2])\n","        qml.CNOT(wires=[1,2])\n","        for q in range(num_qubits):\n","            qml.RY(params_reshaped[idx,q], wires=q)\n","            qml.RZ(params_reshaped[idx+1,q], wires=q)\n","        idx+=2\n","    return qml.state()\n","\n","@lru_cache(None)\n","def get_cached_state(params_tuple):\n","    arr = pnp.array(params_tuple, dtype=pnp.float64)\n","    return circuit_state(arr)\n","\n","def quantum_fidelity_kernel(X, Y):\n","    X = np.atleast_2d(X)\n","    Y = np.atleast_2d(Y)\n","    N, D_ = X.shape\n","    M, D2_ = Y.shape\n","    assert D_==D2_, \"Dimension mismatch\"\n","    K = np.zeros((N,M), dtype=float)\n","    for i in range(N):\n","        xi_tuple = tuple(X[i])\n","        psi_i = get_cached_state(xi_tuple)\n","        for j in range(M):\n","            yj_tuple = tuple(Y[j])\n","            psi_j = get_cached_state(yj_tuple)\n","            overlap = np.vdot(psi_i, psi_j)\n","            val = abs(overlap)**2\n","            K[i,j]=val\n","    return K\n","\n","##############################################################################\n","# 4) Implement a standard EI function (like in Bayesian Opt).\n","#    We'll do a naive negative-EI for LBFGS, so we'll define:\n","##############################################################################\n","def gp_ei(x, f_best, gp):\n","    \"\"\"\n","    Return negative-EI for the point x, so we can do minimization => we want to\n","    minimize -EI => maximize EI\n","    \"\"\"\n","    # x shape => (D,)\n","    x = x[None,:]  # shape(1,D)\n","    mu, std = gp.predict(x, return_std=True)\n","    mu, std = mu[0], std[0]\n","    eps=1e-12\n","    if std<eps:\n","        return 0.0  # or -0 => no improvement\n","    z=(f_best - mu)/std\n","    pdf=1./np.sqrt(2.*math.pi)*math.exp(-0.5*z**2)\n","    cdf=0.5*(1.+erf(z/np.sqrt(2.)))\n","    improvement = (f_best - mu)*cdf + std*pdf\n","    if improvement<0.0:\n","        improvement=0.0\n","    return -improvement  # negative => we'll do minimize\n","\n","def gp_ei_grad(x, f_best, gp):\n","    \"\"\"\n","    We'll do naive finite difference gradient for shape(D,).\n","    x => shape(D,)\n","    We'll do step=1e-4 or so => partial derivative of gp_ei w.r.t x[d].\n","    This is an example. It's possibly expensive for D=24 but demonstrates the approach.\n","    \"\"\"\n","    grad = np.zeros_like(x)\n","    fx = gp_ei(x, f_best, gp)\n","    step=1e-4\n","    for d in range(len(x)):\n","        old_val = x[d]\n","        x[d] = old_val+step\n","        fplus = gp_ei(x, f_best, gp)\n","        x[d] = old_val\n","        grad[d] = (fplus - fx)/step\n","    return grad\n","\n","##############################################################################\n","# 5) The iterative Bayesian Optimization approach with L-BFGS to refine candidate\n","##############################################################################\n","def bo_with_lbfgs(\n","    kernel_fn=quantum_fidelity_kernel,\n","    n_init=5,\n","    n_iter=15,\n","    seed=0,\n","    n_cand=1000\n","):\n","    np.random.seed(seed)\n","    # 1) gather initial data\n","    X_data = np.random.rand(n_init, D)*2.*math.pi\n","    y_data = np.array([cost_fn(row) for row in X_data])\n","\n","    gp = GaussianProcessRegressor(kernel=kernel_fn, alpha=1e-9)\n","    gp.fit(X_data, y_data)\n","\n","    best_hist=[y_data.min()]\n","\n","    for step in range(n_iter):\n","        # f_best\n","        f_best = y_data.min()\n","        # 2) do small random search => pick best from 1000\n","        X_cand = np.random.rand(n_cand, D)*2.*math.pi\n","        ei_vals=[]\n","        for row in X_cand:\n","            ei_vals.append(-gp_ei(row, f_best, gp))  # we store negativeEI to see how large\n","        ei_vals=np.array(ei_vals)\n","        idx=np.argmax(ei_vals) # min of negative => max of EI\n","        x0 = X_cand[idx].copy()\n","\n","        # 3) run LBFGS from x0 to refine\n","        def fun_and_grad(z):\n","            val = gp_ei(z, f_best, gp)\n","            g = gp_ei_grad(z, f_best, gp)\n","            return val, g\n","        bounds=[(0.,2.*math.pi)]*D\n","        res = minimize(fun_and_grad, x0, method='L-BFGS-B', jac=True, bounds=bounds,\n","                       options={'maxiter':30})\n","        x_next=res.x\n","        y_next=cost_fn(x_next)\n","\n","        # 4) update GP\n","        X_data = np.vstack([X_data, x_next])\n","        y_data = np.concatenate([y_data, [y_next]])\n","        gp.fit(X_data, y_data)\n","\n","        current_best=min(best_hist[-1], y_next)\n","        best_hist.append(current_best)\n","        print(f\"Iteration {step+1:2d}, best so far = {current_best:.6f}, y_next={y_next:.6f}\")\n","\n","    return X_data, y_data, best_hist\n","\n","##############################################################################\n","# 6) Run\n","##############################################################################\n","if __name__==\"__main__\":\n","    Xd, yd, bhist = bo_with_lbfgs(kernel_fn=quantum_fidelity_kernel,\n","                                  n_init=5,\n","                                  n_iter=200,\n","                                  seed=None,\n","                                  n_cand=200)\n","    print(\"Final best energy found:\", bhist[-1])\n","    plt.plot(bhist, label=\"Best so far\")\n","    plt.xlabel(\"Iteration\")\n","    plt.ylabel(\"Energy\")\n","    plt.title(\"BO with Quantum Fidelity Kernel + LBFGS Refinement\")\n","    plt.legend()\n","    plt.show()\n"]},{"cell_type":"markdown","metadata":{"id":"GQm_782Z-oNM"},"source":["# GP LFBGS with different kernel, Final Experiment for running"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fkjdk7AI-oQ0"},"outputs":[],"source":["##############################################################################\n","# Imports\n","##############################################################################\n","import numpy as np\n","import math\n","import matplotlib.pyplot as plt\n","from functools import lru_cache\n","from scipy.optimize import minimize\n","from scipy.special import erf\n","\n","import pennylane as qml\n","from pennylane import numpy as pnp\n","\n","##############################################################################\n","# 1) GaussianProcessRegressor (unchanged)\n","##############################################################################\n","class GaussianProcessRegressor:\n","    \"\"\"\n","    Gaussian Process Regressor for d-dimensional inputs.\n","    Returns both predictive mean and std if requested.\n","    \"\"\"\n","    def __init__(self, kernel, alpha=1e-5):\n","        self.kernel = kernel\n","        self.alpha = alpha\n","        self.X_train = None\n","        self.y_train = None\n","        self.K_inv = None\n","\n","    def fit(self, X, y):\n","        X = np.array(X)\n","        y = np.array(y)\n","        self.X_train = X\n","        self.y_train = y\n","        K = self.kernel(X, X)\n","        K += self.alpha * np.eye(len(X))\n","        self.K_inv = np.linalg.inv(K)\n","\n","    def predict(self, X_test, return_std=False):\n","        X_test = np.array(X_test)\n","        K_star = self.kernel(X_test, self.X_train)\n","        y_mean = K_star @ (self.K_inv @ self.y_train)\n","\n","        if return_std:\n","            K_star_star = self.kernel(X_test, X_test)\n","            cov = K_star_star - K_star @ self.K_inv @ K_star.T\n","            var = np.diag(cov)\n","            var = np.maximum(var, 0.0)\n","            y_std = np.sqrt(var)\n","            return y_mean, y_std\n","        else:\n","            return y_mean\n","\n","##############################################################################\n","# 2) Define 3-qubit circuit (24 params) + cost function\n","#    (Use num_layers=3 => D=24)\n","##############################################################################\n","num_qubits = 3\n","num_layers = 1\n","D = (2 + 2*num_layers)*num_qubits  # => (2+2*3)*3=8*3=24\n","dev = qml.device(\"default.qubit\", wires=num_qubits, shots=None)\n","\n","##############################################################################\n","# Build the Ising Hamiltonian: sum_j X_j X_{j+1} + sum_j Z_j\n","##############################################################################\n","obs = []\n","coeffs = []\n","# Coupling X_j X_{j+1}\n","for j in range(num_qubits - 1):\n","    obs.append(qml.PauliX(j) @ qml.PauliX(j+1))\n","    coeffs.append(1.0)\n","# Local Z\n","for j in range(num_qubits):\n","    obs.append(qml.PauliZ(j))\n","    coeffs.append(1.0)\n","\n","H = qml.Hamiltonian(coeffs, obs)\n","\n","@qml.qnode(dev)\n","def circuit_energy(params):\n","    \"\"\"\n","    Prepare a 3-qubit circuit with num_layers=3 => 24 parameters.\n","    Return expectation of H.\n","    \"\"\"\n","    params_reshaped = params.reshape((2*(num_layers+1), num_qubits))\n","    # initial block\n","    for q in range(num_qubits):\n","        qml.RY(params_reshaped[0, q], wires=q)\n","        qml.RZ(params_reshaped[1, q], wires=q)\n","\n","    idx = 2\n","    for _layer in range(num_layers):\n","        # Entangling\n","        qml.CNOT(wires=[0, 1])\n","        qml.CNOT(wires=[0, 2])\n","        qml.CNOT(wires=[1, 2])\n","        # Single-qubit rotations\n","        for q in range(num_qubits):\n","            qml.RY(params_reshaped[idx, q], wires=q)\n","            qml.RZ(params_reshaped[idx+1, q], wires=q)\n","        idx += 2\n","\n","    return qml.expval(H)\n","\n","def cost_fn(params):\n","    return float(circuit_energy(params))\n","\n","##############################################################################\n","# 3) Partial-density QNode + partial_density_kernel\n","##############################################################################\n","@qml.qnode(dev)\n","def circuit_partial_density(params, subset_wires):\n","    \"\"\"\n","    Returns the partial density matrix for a specified subset of qubits.\n","    The rest are traced out automatically by PennyLane.\n","    \"\"\"\n","    params_reshaped = params.reshape((2*(num_layers+1), num_qubits))\n","    # same circuit\n","    for q in range(num_qubits):\n","        qml.RY(params_reshaped[0, q], wires=q)\n","        qml.RZ(params_reshaped[1, q], wires=q)\n","    idx = 2\n","    for _layer in range(num_layers):\n","        qml.CNOT(wires=[0,1])\n","        qml.CNOT(wires=[0,2])\n","        qml.CNOT(wires=[1,2])\n","        for q in range(num_qubits):\n","            qml.RY(params_reshaped[idx, q], wires=q)\n","            qml.RZ(params_reshaped[idx+1, q], wires=q)\n","        idx += 2\n","\n","    return qml.density_matrix(wires=subset_wires)\n","\n","@lru_cache(None)\n","def get_partial_density(params_tuple, subset_wires_tuple):\n","    \"\"\"\n","    Cached partial density matrix.\n","    \"\"\"\n","    arr = pnp.array(params_tuple, dtype=pnp.float64)\n","    dm = circuit_partial_density(arr, list(subset_wires_tuple))\n","    return dm\n","\n","def partial_density_kernel(X, Y, subset_wires=[0], method=\"hilbert_schmidt\"):\n","    \"\"\"\n","    Projected kernel on specified subset of qubits.\n","    For each x_i,y_j => partial density matrices => dm_x, dm_y => Tr(dm_x dm_y).\n","    \"\"\"\n","    X = np.atleast_2d(X)\n","    Y = np.atleast_2d(Y)\n","    N, dx = X.shape\n","    M, dx2 = Y.shape\n","    assert dx == dx2, \"Dimension mismatch\"\n","\n","    wires_tuple = tuple(subset_wires)\n","    K = np.zeros((N, M), dtype=float)\n","\n","    for i in range(N):\n","        x_tuple = tuple(X[i])\n","        dm_x = get_partial_density(x_tuple, wires_tuple)\n","        for j in range(M):\n","            y_tuple = tuple(Y[j])\n","            dm_y = get_partial_density(y_tuple, wires_tuple)\n","            # Hilbert-Schmidt => Tr(dm_x dm_y)\n","            val = np.real(np.trace(dm_x @ dm_y))\n","            K[i, j] = val\n","    return K\n","\n","##############################################################################\n","# 4) We'll define multiple partial kernels for demonstration\n","##############################################################################\n","def kernel_fn1(X, Y):\n","    # single-qubit overlap on qubit 0\n","    return partial_density_kernel(X, Y, subset_wires=[0], method=\"hilbert_schmidt\")\n","\n","def kernel_fn2(X, Y):\n","    # sum of partial overlaps on qubits 0 and 1\n","    return (partial_density_kernel(X, Y, subset_wires=[0]) +\n","           partial_density_kernel(X, Y, subset_wires=[1]))\n","\n","def kernel_fn3(X, Y):\n","    # sum of partial overlaps on qubits 0,1,2 separately\n","    return (partial_density_kernel(X, Y, subset_wires=[0]) +\n","            partial_density_kernel(X, Y, subset_wires=[1]) +\n","            partial_density_kernel(X, Y, subset_wires=[2]))\n","\n","def kernel_fn4(X, Y):\n","    # partial overlap on subset [0,1]\n","    return partial_density_kernel(X, Y, subset_wires=[0,1])\n","\n","def kernel_fn5(X, Y):\n","    return (partial_density_kernel(X, Y, subset_wires=[0,1]) +\n","            partial_density_kernel(X, Y, subset_wires=[1,2]))\n","\n","def kernel_fn6(X, Y):\n","    return (partial_density_kernel(X, Y, subset_wires=[0,1]) +\n","            partial_density_kernel(X, Y, subset_wires=[1,2]) +\n","            partial_density_kernel(X, Y, subset_wires=[0,2]))\n","\n","def kernel_fn7(X, Y):\n","    # full (no partial trace) => subset_wires=[0,1,2]\n","    return partial_density_kernel(X, Y, subset_wires=[0,1,2])\n","\n","KERNELS = [kernel_fn1, kernel_fn2, kernel_fn3, kernel_fn4, kernel_fn5, kernel_fn6, kernel_fn7]\n","\n","##############################################################################\n","# 5) Define EI + gradient for LBFGS (from second snippet, unchanged logic)\n","##############################################################################\n","def gp_ei(x, f_best, gp):\n","    \"\"\"\n","    Return negative-EI for the point x => we do 'minimize' =>\n","    so negative-EI is what we minimize to find maximum of EI.\n","    \"\"\"\n","    x = x[None, :]  # shape (1,D)\n","    mu, std = gp.predict(x, return_std=True)\n","    mu, std = mu[0], std[0]\n","    eps = 1e-12\n","    if std < eps:\n","        # No improvement if variance is tiny\n","        return 0.0\n","    z = (f_best - mu)/std\n","    pdf = 1./np.sqrt(2.*math.pi)*math.exp(-0.5*z**2)\n","    cdf = 0.5*(1.+erf(z/np.sqrt(2.)))\n","    improvement = (f_best - mu)*cdf + std*pdf\n","    if improvement < 0.0:\n","        improvement = 0.0\n","    return -improvement  # negative => we'll do minimize\n","\n","def gp_ei_grad(x, f_best, gp):\n","    \"\"\"\n","    Naive finite-difference gradient for gp_ei.\n","    x => shape(D,)\n","    \"\"\"\n","    grad = np.zeros_like(x)\n","    fx = gp_ei(x, f_best, gp)\n","    step = 1e-4\n","    for d in range(len(x)):\n","        old_val = x[d]\n","        x[d] = old_val + step\n","        fplus = gp_ei(x, f_best, gp)\n","        x[d] = old_val\n","        grad[d] = (fplus - fx)/step\n","    return grad\n","\n","##############################################################################\n","# 6) bo_with_lbfgs => Main iterative routine that uses partial kernel\n","##############################################################################\n","def bo_with_lbfgs(\n","    kernel_fn,\n","    n_init=5,\n","    n_iter=50,\n","    seed=0,\n","    n_cand=100\n","):\n","    \"\"\"\n","    Perform Bayesian Optimization with a given kernel_fn,\n","    using L-BFGS to refine each new candidate.\n","    \"\"\"\n","    np.random.seed(seed)\n","    # 1) gather initial data\n","    X_data = np.random.rand(n_init, D)*2.*math.pi\n","    y_data = np.array([cost_fn(row) for row in X_data])\n","\n","    gp = GaussianProcessRegressor(kernel=kernel_fn, alpha=1e-9)\n","    gp.fit(X_data, y_data)\n","\n","    best_hist = [y_data.min()]\n","\n","    for step in range(n_iter):\n","        # current best\n","        f_best = y_data.min()\n","        # 2) do random search => pick best among n_cand\n","        X_cand = np.random.rand(n_cand, D)*2.*math.pi\n","        ei_vals = []\n","        for row in X_cand:\n","            # store negativeEI => we want largest positive EI\n","            ei_vals.append(-gp_ei(row, f_best, gp))\n","        ei_vals = np.array(ei_vals)\n","        idx = np.argmax(ei_vals)\n","        x0 = X_cand[idx].copy()\n","\n","        # 3) refine with L-BFGS\n","        def fun_and_grad(z):\n","            val = gp_ei(z, f_best, gp)\n","            g = gp_ei_grad(z, f_best, gp)\n","            return val, g\n","\n","        bounds = [(0., 2.*math.pi)]*D\n","        res = minimize(fun_and_grad, x0, method='L-BFGS-B',\n","                       jac=True, bounds=bounds,\n","                       options={'maxiter': 30})\n","\n","        x_next = res.x\n","        y_next = cost_fn(x_next)\n","\n","        # 4) update GP\n","        X_data = np.vstack([X_data, x_next])\n","        y_data = np.concatenate([y_data, [y_next]])\n","        gp.fit(X_data, y_data)\n","\n","        current_best = min(best_hist[-1], y_next)\n","        best_hist.append(current_best)\n","        print(f\"Iteration {step+1:2d}, best so far = {current_best:.6f}, y_next={y_next:.6f}\")\n","\n","    return X_data, y_data, best_hist\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xrQQL9J8AO2f"},"outputs":[],"source":["import numpy as np\n","import math\n","\n","def full_kernel_classic(x,y):\n","\n","    #print(x,y)\n","\n","    return kernel_fn7(np.array([x]),np.array([y]))[0][0]\n","\n","def p_greedy_newton_flexible(kernel_func, Omega_b, max_m):\n","    \"\"\"\n","    P-greedy on a discrete set Omega_b, up to max_m basis functions.\n","\n","    Returns a dict pgreedy_info with:\n","      - 'm': number of basis chosen (= max_m, unless Omega_b empty)\n","      - 'center_pts': array of shape (m, d) for the chosen centers\n","      - 'chosen_indices': the indices in Omega_b for each chosen center\n","      - 'Nvals': shape (m, nB), Nvals[k, i] = N_{k+1}(Omega_b[i])\n","      - 'denominators': shape (m,), the normalization used at each step\n","\n","    Using this info, we can evaluate the Newton basis for new points x.\n","    \"\"\"\n","    nB = len(Omega_b)\n","    if nB == 0 or max_m == 0:\n","        return {\n","            'm': 0,\n","            'center_pts': np.zeros((0,0)),\n","            'chosen_indices': [],\n","            'Nvals': np.zeros((0,0)),\n","            'denominators': np.zeros(0)\n","        }\n","\n","    # Precompute diagonal K(x_i,x_i)\n","    diagK = np.array([kernel_func(Omega_b[i],Omega_b[i]) for i in range(nB)])\n","    Nvals = np.zeros((max_m, nB))  # Nvals[k, i] => N_{k+1}(Omega_b[i])\n","    denominators = np.zeros(max_m)\n","    chosen_indices = []\n","    center_pts = []\n","\n","    # 1) pick first center\n","    i1 = np.argmax(diagK)\n","    chosen_indices.append(i1)\n","    center_pts.append(Omega_b[i1])\n","\n","    denom0 = math.sqrt(diagK[i1]) if diagK[i1]>1e-14 else 1e-14\n","    denominators[0] = denom0\n","\n","    # define N_1(x_i)\n","    for i in range(nB):\n","        #print(Omega_b[i].shape, Omega_b[i1].shape)\n","        Nvals[0, i] = kernel_func(Omega_b[i], Omega_b[i1]) / denom0\n","\n","    # power function squared\n","    P2 = diagK - Nvals[0,:]**2\n","\n","    m=1\n","    while m<max_m:\n","        # print(diagK.shape)\n","        # print(Nvals.shape)\n","        # print(P2.shape)\n","        i_next = np.argmax(P2)\n","        chosen_indices.append(i_next)\n","        center_pts.append(Omega_b[i_next])\n","\n","        denom = math.sqrt(P2[i_next]) if P2[i_next]>1e-14 else 1e-14\n","        denominators[m] = denom\n","\n","        for i in range(nB):\n","            #print(Omega_b[i].shape, Omega_b[i_next].shape)\n","            val = kernel_func(Omega_b[i], Omega_b[i_next])\n","            # subtract sum_{r=0..m-1} Nvals[r, i_next]*Nvals[r, i]\n","            tmp=0.0\n","            for r in range(m):\n","                tmp += Nvals[r, i_next]*Nvals[r, i]\n","            val -= tmp\n","            Nvals[m, i] = val/denom\n","\n","        P2 = P2 - Nvals[m,:]**2\n","        m+=1\n","\n","    center_pts = np.array(center_pts)  # shape (m,d)\n","\n","    pgreedy_info = {\n","        'm': m,\n","        'center_pts': center_pts,\n","        'chosen_indices': chosen_indices,\n","        'Nvals': Nvals,\n","        'denominators': denominators\n","    }\n","    return pgreedy_info\n","\n","def evaluate_newton_basis(x, kernel_func, pgreedy_info, Omega_b):\n","    \"\"\"\n","    For a new point x, compute [N_1(x), ..., N_m(x)] using the chosen centers from pgreedy_info.\n","    This replicates the recursion used in p_greedy_newton_flexible for the discrete set,\n","    but for x in continuous space.\n","\n","    We need:\n","      - center_pts[k] = the chosen center for the (k+1)-th basis\n","      - denominators[k]\n","      - Nvals[r, i_k] = N_r(center_pts[k]) if i_k= chosen_indices[k]\n","    \"\"\"\n","    m = pgreedy_info['m']\n","    if m==0:\n","        return np.zeros(0)\n","    center_pts = pgreedy_info['center_pts']\n","    chosen_indices = pgreedy_info['chosen_indices']\n","    Nvals = pgreedy_info['Nvals']\n","    denominators = pgreedy_info['denominators']\n","\n","    newton_x = np.zeros(m)\n","\n","    # 1) N_1(x)\n","    val = kernel_func(x, center_pts[0])\n","    newton_x[0] = val/denominators[0]\n","\n","    # 2) subsequent\n","    for k in range(1,m):\n","        val = kernel_func(x, center_pts[k])\n","\n","        # subtract sum_{r=0..k-1} [N_r(center_pts[k]) * N_r(x)]\n","        # but N_r(center_pts[k]) = Nvals[r, chosen_indices[k]]\n","        tmp = 0.0\n","        i_k = chosen_indices[k]\n","        for r in range(k):\n","            tmp += Nvals[r, i_k]*newton_x[r]\n","\n","        val -= tmp\n","        newton_x[k] = val / denominators[k]\n","\n","    return newton_x\n","\n","def approximate_kernel_flexible(x, y, kernel_func, pgreedy_info, Omega_b):\n","    \"\"\"\n","    approximate kernel = sum_{k=1..m} N_k(x)*N_k(y).\n","    We'll compute N_k(x) and N_k(y) on the fly.\n","    \"\"\"\n","    Nx = evaluate_newton_basis(x, kernel_func, pgreedy_info, Omega_b)\n","    Ny = evaluate_newton_basis(y, kernel_func, pgreedy_info, Omega_b)\n","    return np.dot(Nx, Ny)\n","\n","###########################################################################\n","# Example usage\n","###########################################################################\n","\n","\n","\n","if __name__==\"__main__\":\n","    # Suppose we have a discrete set Omega_b in [0,1]^3 (or [0, pi]^3).\n","    # We'll just create a random set for demonstration.\n","    np.random.seed(None)\n","    d=12\n","    nB=1000\n","    Omega_b = np.random.rand(nB,d)*2*math.pi\n","\n","    # We'll run P-greedy to get m=10 basis\n","    max_m = 10\n","    pgreedy_info = p_greedy_newton_flexible(full_kernel_classic, Omega_b, max_m)\n","\n","    # Now pick some \"new\" points x,y that might not be in Omega_b\n","    x_test = np.random.rand(d) * 2 * math.pi\n","    y_test = np.random.rand(d) * 2 * math.pi\n","\n","    # Evaluate approximate kernel\n","    k_approx = approximate_kernel_flexible(x_test, y_test, full_kernel_classic, pgreedy_info, Omega_b)\n","\n","    # Compare to true kernel\n","    k_true = full_kernel_classic(x_test, y_test)\n","\n","    print(f\"Approx kernel(x_test,y_test) = {k_approx:.5f}\")\n","    print(f\"True  kernel(x_test,y_test)  = {k_true:.5f}\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7_o83YIpFhyh"},"outputs":[],"source":["x_test = np.random.rand(d) * 2 * math.pi\n","y_test = np.random.rand(d) * 2 * math.pi\n","\n","# Evaluate approximate kernel\n","k_approx = approximate_kernel_flexible(x_test, y_test, full_kernel_classic, pgreedy_info, Omega_b)\n","\n","# Compare to true kernel\n","k_true = full_kernel_classic(x_test, y_test)\n","\n","print(f\"Approx kernel(x_test,y_test) = {k_approx:.5f}\")\n","print(f\"True  kernel(x_test,y_test)  = {k_true:.5f}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vUY5PSp_FmDH"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"znkHG9lVAPZd"},"outputs":[],"source":["##############################################################################\n","# 7) Example Usage\n","##############################################################################\n","if __name__==\"__main__\":\n","    # Example: pick any partial kernel, e.g. kernel_fn4 => subset_wires=[0,1]\n","    # Or pick among KERNELS = [kernel_fn1,...,kernel_fn7].\n","    # We'll do a small run for demonstration.\n","\n","    dim = 100\n","    Omega_b = np.random.rand(1000,12)*2*math.pi\n","\n","    max_m = dim\n","    pgreedy_info = p_greedy_newton_flexible(full_kernel_classic, Omega_b, max_m)\n","    approx_kernel = lambda x,y: approximate_kernel_flexible(x,y, full_kernel_classic, pgreedy_info, Omega_b)\n","\n","    def kernell(X,Y):\n","        K = np.zeros((X.shape[0],Y.shape[0]))\n","        for i in range(X.shape[0]):\n","          for j in range(Y.shape[0]):\n","            K[i,j] = approx_kernel(X[i],Y[j])\n","        return K\n","\n","    X_data, y_data, best_hist = bo_with_lbfgs(\n","        kernel_fn=kernell,\n","        n_init=5,\n","        n_iter=30,\n","        seed=None,\n","        n_cand=10\n","    )\n","    print(\"Final best energy found:\", best_hist[-1])\n","\n","    # Plot\n","    plt.plot(best_hist, label=\"Best so far\")\n","    plt.xlabel(\"Iteration\")\n","    plt.ylabel(\"Energy\")\n","    plt.title(\"BO with Partial-Density Kernel + LBFGS (subset [0,1])\")\n","    plt.legend()\n","    plt.show()\n","\n","    # If you want to loop over all partial kernels, you could do:\n","    # for i, kfn in enumerate(KERNELS):\n","    #     Xd, yd, bh = bo_with_lbfgs(kfn, n_init=3, n_iter=15, seed=42)\n","    #     print(f\"Kernel #{i+1}: final best energy = {bh[-1]}\")"]},{"cell_type":"markdown","metadata":{"id":"N-jS684IAPht"},"source":["## repeated experiments"]},{"cell_type":"code","source":["\n","import numpy as np\n","from numpy.linalg import matrix_rank, svd\n","\n","def kernel_rank(K, rtol=1e-12):\n","    \"\"\"\n","    Return the numerical rank of a kernel matrix K.\n","\n","    Parameters\n","    ----------\n","    K : ndarray, shape (n_samples, n_samples)\n","        Symmetric (or Hermitian) kernel Gram matrix.\n","    rtol : float\n","        Relative threshold for considering singular values to be zero.\n","        Default `rtol=1e-12` ≃ np.finfo(float).eps ** 0.5.\n","\n","    Returns\n","    -------\n","    rank : int\n","        Numerical rank of K.\n","    \"\"\"\n","    # Option 1 – wrapper around NumPy’s built-in rank\n","    # (uses SVD under the hood, with its own tolerance)\n","    builtin_rank = matrix_rank(K, tol=rtol)\n","\n","    # Option 2 – explicit SVD, lets you tune the threshold yourself\n","    s = svd(K, compute_uv=False)              # singular values (non-negative)\n","    thresh = rtol * s.max()                   # same criterion NumPy uses\n","    manual_rank = int((s > thresh).sum())     # count non-negligible values\n","\n","    # They should be identical; return either\n","    assert builtin_rank == manual_rank\n","    return builtin_rank\n"],"metadata":{"id":"mWonYS7727OQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["Omega_b = np.random.rand(100,12)*2*math.pi\n","kernel_rank(kernel_fn7(Omega_b,Omega_b))"],"metadata":{"id":"NQPH4xDc2nK0"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zcHLlDIC_gK2"},"outputs":[],"source":["if __name__==\"__main__\":\n","    # Example usage: define partial kernel on qubits [0,2]\n","    # Then do BO\n","\n","    trials = 30\n","    energy_final = []\n","    energy_final_std = []\n","\n","    dims = np.array([2,4,8,16,32,64])\n","\n","    for dim in dims:\n","        energies = []\n","        for trial in range(trials):\n","            Omega_b = np.random.rand(1000,12)*2*math.pi\n","\n","            max_m = dim\n","            pgreedy_info = p_greedy_newton_flexible(full_kernel_classic, Omega_b, max_m)\n","            approx_kernel = lambda x,y: approximate_kernel_flexible(x,y, full_kernel_classic, pgreedy_info, Omega_b)\n","\n","            def kernell(X,Y):\n","                K = np.zeros((X.shape[0],Y.shape[0]))\n","                for i in range(X.shape[0]):\n","                  for j in range(Y.shape[0]):\n","                    K[i,j] = approx_kernel(X[i],Y[j])\n","                return K\n","\n","\n","            X_data, y_data, best_hist = bo_with_lbfgs(\n","                kernel_fn=kernell,\n","                n_init=5,\n","                n_iter=50,\n","                seed=None,\n","                n_cand=10\n","            )\n","            energies.append(best_hist)\n","            print(f\"dim {dim} completed trial {trial}\")\n","            print(\"Final best energy found:\", best_hist[-1])\n","        energy_all = np.array(energies)\n","        avg_energy = energy_all.mean(axis=0)\n","        std_energy = energy_all.std(axis=0)\n","\n","        energy_final.append(avg_energy[-1])\n","        energy_final_std.append(std_energy[-1])\n","\n","\n","\n","    plt.errorbar(dims, np.array(energy_final), np.array(energy_final_std), ls=\"-\",\n","                marker='d',\n","                color=\"#009E73\",\n","                alpha=1.0,\n","                capsize=4)"]}],"metadata":{"colab":{"collapsed_sections":["Xe15VI0eXqSk","4ql30DbvXlHF"],"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}