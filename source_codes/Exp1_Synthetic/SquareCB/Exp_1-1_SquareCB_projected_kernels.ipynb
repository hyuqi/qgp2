{"cells":[{"cell_type":"markdown","metadata":{"id":"k_Ch3AEZr_HP"},"source":["# Helper Function"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ff_B0fAhKrND"},"outputs":[],"source":["\"\"\"\n","This module contains all the functions that define the kernels\n","\"\"\"\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import torch\n","\n","\n","def full_kernel_classic(x, y):\n","    \"\"\"\n","    Classical evaluation of the full kernel\n","    :param x: arg 1\n","    :param y: arg 2\n","    :return:\n","    \"\"\"\n","    k = 1\n","    for i in range(len(x)):\n","        k = k * np.cos(1/2*(x[i]-y[i]))**2\n","    return k\n","\n","\n","def kernel_matrix_classic(X, Y):\n","    \"\"\"\n","    compute the kenel matrix of the full kernel\n","    :param X: vector of samples\n","    :param Y: vector of samples\n","    :return:\n","    \"\"\"\n","    K = np.zeros((len(X), len(Y)))\n","    for i in range(len(X)):\n","        for j in range(len(Y)):\n","            K[i, j] = full_kernel_classic(X[i], Y[j])\n","    return K\n","\n","\n","def kernel_matrix_classic_torch(X, Y):\n","    \"\"\"\n","    compute the kenel matrix of the full kernel potentially utilizing some parallel processing\n","    :param X: vector of samples\n","    :param Y: vector of samples\n","    :return:\n","    \"\"\"\n","    if type(X) is np.ndarray:\n","        X = torch.from_numpy(X)\n","    if type(Y) is np.ndarray:\n","        Y = torch.from_numpy(Y)\n","    # create tensor with entry i x j x k equal to x_ik - y_jk\n","    X = X.unsqueeze(1).expand(-1, Y.size(0), -1)\n","    Y = Y.unsqueeze(0).expand(X.size(0), -1, -1)\n","    K = X - Y\n","    K = torch.cos(K / 2) ** 2\n","    K = torch.prod(K, 2)\n","    return K\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5ULe1zRLh8-I"},"outputs":[],"source":["import numpy as np\n","\n","class GaussianProcessRegressor:\n","    \"\"\"\n","    Gaussian Process Regressor for d-dimensional inputs.\n","    Returns both predictive mean and std if requested.\n","    \"\"\"\n","    def __init__(self, kernel, alpha=1e-5):\n","        self.kernel = kernel\n","        self.alpha = alpha\n","        self.X_train = None\n","        self.y_train = None\n","        self.K_inv = None\n","\n","    def fit(self, X, y):\n","        X = np.array(X)\n","        y = np.array(y)\n","        self.X_train = X\n","        self.y_train = y\n","        K = self.kernel(X, X)\n","        K += self.alpha * np.eye(len(X))\n","        self.K_inv = np.linalg.inv(K)\n","\n","    def predict(self, X_test, return_std=False):\n","\n","        X_test = np.array(X_test)\n","        K_star = self.kernel(X_test, self.X_train)\n","        y_mean = K_star @ (self.K_inv @ self.y_train)\n","\n","        if return_std:\n","            K_star_star = self.kernel(X_test, X_test)\n","            cov = K_star_star - K_star @ self.K_inv @ K_star.T\n","            var = np.diag(cov)\n","            var = np.maximum(var, 0.0)\n","            y_std = np.sqrt(var)\n","            return y_mean, y_std\n","        else:\n","            return y_mean\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6EkvzaE6bmOU"},"outputs":[],"source":["from itertools import product\n","\n","def build_grid(bounds, n_grid):\n","    \"\"\"\n","    Build a uniform grid of points within the given 'bounds'.\n","\n","    Parameters\n","    ----------\n","    bounds : list of (low, high) for each dimension (length d)\n","    n_grid : int\n","        Number of grid points per dimension.\n","\n","    Returns\n","    -------\n","    X_grid : np.ndarray of shape (n_grid^d, d)\n","        All points in the grid.\n","    \"\"\"\n","    # For each dimension, create an array of n_grid points from low to high\n","    axes = [np.linspace(low, high, n_grid) for (low, high) in bounds]\n","    # Cartesian product of all axes\n","    # e.g. for d=2, we get all pairs (x,y); for d=3, all (x,y,z), etc.\n","    mesh = list(product(*axes))  # a list of d-tuples\n","    return np.array(mesh)\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zUirsK3YG8nr"},"outputs":[],"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","from itertools import product\n","from scipy.stats import norm\n","\n","\n","\n","def SquareCB_nd(\n","    f,                 # black-box reward function, shape: (n, d) -> (n,)\n","    gp,                # a GaussianProcessRegressor (our \"oracle\")\n","    bounds,            # list of (low, high) for each dimension\n","    n_iter=100,         # number of SquareCB iterations\n","    init_points=3,     # number of initial random samples\n","    n_grid=5,         # discrete set of arms = the grid\n","    gamma=1.0,         # SquareCB learning-rate parameter\n","    mu=None,           # exploration offset, default = K (the number of grid points)\n","    random_state=None,\n","    noise_std=0.1,     # optional: noise level for synthetic reward observation\n","    verbose=False,\n","    beta_func = None,\n","    error = 1\n","):\n","    \"\"\"\n","    Illustrative 'SquareCB' variant for Bayesian Opt. with a GP 'oracle' + discrete grid.\n","\n","    Parameters\n","    ----------\n","    f : callable\n","        The unknown reward function mapping array (n,d)->(n,).\n","    gp : GaussianProcessRegressor\n","        Must have .fit(X, y) and .predict(X, return_std=...) etc.\n","        We treat gp as our 'online regression' oracle for the paper's SquareCB.\n","    bounds : list of (low, high)\n","        The domain bounding box in d dimensions.\n","    n_iter : int\n","        Number of SquareCB rounds.\n","    init_points : int\n","        How many random points to sample initially (random exploration).\n","    n_grid : int\n","        Grid resolution in each dimension.  => K = n_grid^d arms.\n","    gamma : float\n","        The 'learning rate' parameter from SquareCB.  Usually tuned or derived from theory.\n","    mu : float or None\n","        Exploration offset.  If None, we set mu = K = n_grid^d.\n","    random_state : int or None\n","        For reproducibility.\n","    noise_std : float\n","        Optional noise standard deviation for the observation of f(x).\n","    verbose : bool\n","        Print iteration details if True.\n","\n","    Returns\n","    -------\n","    regrets : ndarray of shape (n_iter + init_points,)\n","        Cumulative regret at each iteration.\n","    X_samples : ndarray of shape (n_iter + init_points, d)\n","        The chosen points (arms) each round.\n","    Y_samples : ndarray of shape (n_iter + init_points,)\n","        The noisy observed rewards from f.\n","    \"\"\"\n","\n","    if random_state is not None:\n","        np.random.seed(random_state)\n","\n","    d = len(bounds)\n","    # 1) Build the discrete action set via a grid\n","    X_grid = build_grid(bounds, n_grid=n_grid)   # shape (K, d)\n","    K = X_grid.shape[0]                          # total number of arms\n","\n","    if mu is None:\n","        mu = 1#float(K)  # default per the paper's suggestion is mu = #actions\n","\n","    # 2) Approximate global max among the grid for regret computation\n","    Y_grid = f(X_grid)\n","    idx_best = np.argmax(Y_grid)\n","    x_star = X_grid[idx_best]\n","    f_star = Y_grid[idx_best]   # approximate best reward among grid\n","\n","    # 3) Initialize data with some random points from domain\n","    #    (like in your GP-UCB code)\n","    def sample_random(n):\n","        return np.array([\n","            [np.random.uniform(low, high) for (low, high) in bounds]\n","            for _ in range(n)\n","        ])\n","    X_samples = sample_random(init_points)            # (init_points, d)\n","    Y_samples = f(X_samples) + noise_std * np.random.randn(init_points)  # add noise\n","\n","    # 4) Track cumulative regrets\n","    regrets = np.zeros(n_iter + init_points)\n","    cum_regret = 0.0\n","    for i in range(init_points):\n","        cum_regret += (f_star - Y_samples[i])\n","        regrets[i] = cum_regret\n","\n","    # 5) Main loop: 'SquareCB'\n","    for step in range(n_iter):\n","        t = init_points + step + 1  # 1-based iteration index\n","\n","        # (a) Fit the GP on current data (X_samples, Y_samples)\n","        gp.fit(X_samples, Y_samples)\n","\n","        # (b) Predict the reward for each action a in X_grid\n","        #     SquareCB uses \"predicted losses\" but we can equivalently treat negative reward\n","        #     or just pick an \"argmin predicted\" approach. We'll treat \"loss = - predicted_reward.\"\n","        mean_grid, std_grid = gp.predict(X_grid, return_std=True)\n","        np.random.seed(None)\n","        # In classical SquareCB, we want a \"score\" = predicted loss.\n","        # If we only have predicted reward, we can define:\n","        #   y_hat[a] = -mean_grid[a]\n","        # i.e. the \"predicted loss\" = negative predicted reward\n","        y_hat_grid = -mean_grid\n","\n","        # (c) Identify 'best action' b_t = arg min y_hat\n","        b_idx = np.argmin(y_hat_grid)\n","        y_hat_b = y_hat_grid[b_idx]\n","\n","        # (d) For each a != b, define p_t(a) = 1 / [mu + gamma*(y_hat[a] - y_hat_b)]\n","        #     Then p_t(b) = 1 - sum_{a!=b} p_t(a).\n","\n","        mu = K\n","        gamma = K #( K * n_iter / error ) ** 0.5 * 10\n","\n","\n","        p = np.zeros(K, dtype=float)\n","        sum_others = 0.0\n","        for a_idx in range(K):\n","            if a_idx != b_idx:\n","                gap = (y_hat_grid[a_idx] - y_hat_b)\n","                # If gap <= 0, it can be tricky or cause big p. Add small offset if needed\n","                denom = mu + gamma*max(0.0, gap)\n","                p[a_idx] = 1.0 / denom\n","                sum_others += p[a_idx]\n","        p[b_idx] = 1.0 - sum_others\n","        if p[b_idx] < 0.0:\n","            # If numerical issues occur, clip or renormalize\n","            p = np.clip(p, 0, None)\n","            p /= np.sum(p)\n","\n","        #print(f(X_grid),mean_grid,p)\n","        #print(f\"======={step}========\")\n","        # (e) Sample next action x_next from distribution p\n","        a_next = np.random.choice(K, p=p)\n","        x_next = X_grid[a_next].reshape(1, -1)\n","\n","        # (f) Observe reward from f with optional noise\n","        y_true = f(x_next)  # shape (1,)\n","        noise = noise_std * np.random.randn()\n","        y_obs = y_true[0] + noise\n","\n","        # (g) Update data\n","        X_samples = np.vstack([X_samples, x_next])\n","        Y_samples = np.concatenate([Y_samples, [y_obs]])\n","\n","        # (h) Update cumulative regret\n","        cum_regret += (f_star - y_true[0])\n","        regrets[init_points + step] = cum_regret\n","\n","        if verbose:\n","            print(f\"[SquareCB] Round {step+1}/{n_iter}, t={t}, bestArm={b_idx}, \"\n","                  f\"ChosenArm={a_next}, regret={cum_regret:.4f}\")\n","\n","    return regrets, X_samples, Y_samples\n","\n","\n","\n","\n","\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from tqdm import trange\n","\n","def run_multiple_experiments_SquareCB(\n","    f,\n","    gp,\n","    bounds,\n","    n_runs=5,\n","    n_iter=10,\n","    init_points=3,\n","    n_grid=50,\n","    beta_func=None,\n","    random_state=None,\n","    error = 1,\n","    verbose=False\n","):\n","    \"\"\"\n","    Runs the GP-UCB experiment 'n_runs' times, each time creating a new GP instance.\n","    Returns the average cumulative regret across runs.\n","\n","    Parameters\n","    ----------\n","    f : callable\n","        The black-box reward function.\n","    gp_class_factory : callable\n","        A function that, when called, returns a *new* untrained GaussianProcessRegressor\n","        (or similar). For example:\n","          lambda: GaussianProcessRegressor(kernel=RBF(...), alpha=..., optimizer=None)\n","        We need a fresh GP for each run, so that each run is independent.\n","    bounds : list of (low, high)\n","        Domain bounding box.\n","    n_runs : int\n","        Number of independent runs to average over.\n","    n_iter : int\n","        Number of GP-UCB rounds (not counting the init_points).\n","    init_points : int\n","        Number of random initial points in each run.\n","    n_grid : int\n","        Grid resolution for argmax search in each run.\n","    beta_func : callable or None\n","        If None, use a default log-based. Otherwise a function beta_func(t) -> float.\n","    random_state : int or None\n","        For reproducibility. If set, seeds the first run's RNG, then subsequent runs\n","        will shift the seed.\n","    verbose : bool\n","        Whether to print details for each run.\n","\n","    Returns\n","    -------\n","    avg_regret : ndarray of shape (n_iter + init_points,)\n","        The pointwise average of the cumulative regret across runs.\n","    regrets_all : ndarray of shape (n_runs, n_iter + init_points)\n","        The individual run's cumulative-regret curves.\n","    \"\"\"\n","\n","    from copy import deepcopy\n","\n","    # If we want reproducibility, set base seed\n","    base_seed = random_state if random_state is not None else None\n","\n","    # We'll store the regret curve for each run here\n","    regrets_all = []\n","\n","    for i in trange(n_runs, desc=\"GP-UCB Experiments\"):\n","        # For each run, optionally shift the seed\n","        if base_seed is not None:\n","            # shift by i to get distinct seeds\n","            np.random.seed(base_seed + i)\n","\n","        # Create a fresh GP instance\n","        gp_model = gp\n","\n","        # Import or copy the SquareCB_nd from your previous code snippet:\n","        regrets, X_samples, Y_samples = SquareCB_nd(\n","            f=f,\n","            gp=gp_model,\n","            bounds=bounds,\n","            n_iter=n_iter,\n","            init_points=init_points,\n","            n_grid=n_grid,\n","            beta_func=beta_func,\n","            random_state=None,  # we've set the seed externally\n","            error = error,\n","            verbose=(verbose and i == 0)  # only verbose in the 1st run, e.g.\n","        )\n","\n","        regrets_all.append(regrets)\n","\n","    regrets_all = np.array(regrets_all)  # shape (n_runs, n_steps)\n","    avg_regret = regrets_all.mean(axis=0)\n","    std_regret = regrets_all.std(axis=0)\n","    return avg_regret, regrets_all, std_regret\n","\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"j6dho51fuY4N"},"outputs":[],"source":["pip install PennyLane~=0.40.0"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EoBOkDLug9Im"},"outputs":[],"source":["\"\"\"\n","This module contains all the functions that define the kernels\n","\"\"\"\n","import pennylane as qml\n","from pennylane.templates.layers import RandomLayers\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import torch\n","\n","\n","#@qml.template\n","def product_encoding(x, wires=[0]):\n","    assert len(x) == len(wires), 'number of parameters does not match number of qubits'\n","    # encode data via single qubit rotation of each dimension seperately\n","    for i in wires:\n","        # data encoding\n","        qml.RX(x[i], wires=i)\n","\n","\n","def circuit_function(x, data_encoding, qubits, seed, return_reduced_state=False, M=\"Z\", reduced_state=0):\n","    np.random.seed(seed)\n","    wires = [i for i in range(qubits)]\n","    # First encode the data\n","    data_encoding(x, wires=wires)\n","\n","    # Here we define how the random unitary matrix V is created\n","    layers = qubits #** 2  # we use qubits^2 rotations\n","    # create random angles corresponding to drawing V\n","    weights = np.random.uniform(0, 2*np.pi, size=(layers, qubits))\n","    ratio_imprim = 0.5 # use as many CNOTs as rotation gates\n","    RandomLayers(weights=weights, ratio_imprim=ratio_imprim, wires=wires, seed=seed)\n","    # for i in range(qubits-1):\n","    #     qml.CNOT(wires=[i,i+1])\n","\n","    #print(reduced_state)\n","    if return_reduced_state:\n","        # return the reduced state of the single qubit that defines the biased kernel.\n","        return qml.density_matrix(reduced_state)\n","    else:  # return the function f(x)\n","        if M==\"Z\":\n","            #  the observable that defines the target function (f^*) is arbitrarily chosen as the pauli-Z on the first qubit.\n","            return qml.expval(qml.PauliZ(0))\n","        if M==\"0\":\n","            #  you can also chooose the observable |0><0| or implement anything else.\n","            return qml.expval(qml.Hermitian([[1, 0], [0, 0]], wires=0))\n","\n","\n","def full_kernel_fct(x, y, data_encoding):\n","    \"\"\"\n","    defines the full kernel via the simulation of the quantum circuit. for the simple kernel we consider, we directly\n","    evaluate its classical description\n","    :param x: arg 1\n","    :param y: arg 2\n","    :param data_encoding: Define the data encoding unitary\n","    :return:\n","    \"\"\"\n","    # initial state is all zero\n","    qubits = len(x)\n","    wires = [i for i in range(qubits)]  # qubits to work on\n","    # encode first argument\n","    data_encoding(x, wires=wires)\n","    #  inversely encode second argument\n","    qml.inv(data_encoding(y, wires=wires))\n","\n","    # project back onto the all 0 state\n","    projector = np.zeros((2 ** qubits, 2 ** qubits))\n","    projector[0, 0] = 1\n","    return qml.expval(qml.Hermitian(projector, wires=range(qubits)))\n","\n","\n","def full_kernel_classic(x, y):\n","    \"\"\"\n","    Classical evaluation of the full kernel\n","    :param x: arg 1\n","    :param y: arg 2\n","    :return:\n","    \"\"\"\n","    k = 1\n","    for i in range(len(x)):\n","        k = k * np.cos(1/2*(x[i]-y[i]))**2\n","    return k\n","\n","\n","def kernel_matrix_classic(X, Y):\n","    \"\"\"\n","    compute the kenel matrix of the full kernel\n","    :param X: vector of samples\n","    :param Y: vector of samples\n","    :return:\n","    \"\"\"\n","    K = np.zeros((len(X), len(Y)))\n","    for i in range(len(X)):\n","        for j in range(len(Y)):\n","            K[i, j] = full_kernel_classic(X[i], Y[j])\n","    return K\n","\n","\n","def kernel_matrix_classic_torch(X, Y):\n","    \"\"\"\n","    compute the kenel matrix of the full kernel potentially utilizing some parallel processing\n","    :param X: vector of samples\n","    :param Y: vector of samples\n","    :return:\n","    \"\"\"\n","    if type(X) is np.ndarray:\n","        X = torch.from_numpy(X)\n","    if type(Y) is np.ndarray:\n","        Y = torch.from_numpy(Y)\n","    # create tensor with entry i x j x k equal to x_ik - y_jk\n","    X = X.unsqueeze(1).expand(-1, Y.size(0), -1)\n","    Y = Y.unsqueeze(0).expand(X.size(0), -1, -1)\n","    K = X - Y\n","    K = torch.cos(K / 2) ** 2\n","    K = torch.prod(K, 2)\n","    return K\n","\n","\n","def biased_kernel_fct(x, y, reduced_state):\n","    \"\"\"\n","    Compute the biased kernel function with the reduced density matrix rho(x) = reduced_state(x)\n","    :param x:\n","    :param y:\n","    :param reduced_state: function that takes a single argument and returns the a reduced denisty operator\n","    :return:\n","    \"\"\"\n","    # works with the reduced first qubit density operators\n","    rho_x = reduced_state(x)\n","    rho_y = reduced_state(y)\n","    k = np.real(np.trace(rho_x @ rho_y))\n","    return k\n","\n","\n","def biased_kernel_matrix(X, Y, reduced_state):\n","    \"\"\"\n","    Compute the kernel matrix of the biased kernel with torch\n","    :param X: input vector of data\n","    :param Y: input vector of data\n","    :param reduced_state: function that takes a single argument and returns the a reduced denisty operator\n","    :return: kernel matrix\n","    \"\"\"\n","    # works with the reduced density operators\n","    rho_X = torch.tensor([np.array(reduced_state(x)) for x in X]) # compute reduced density operator for all inputs\n","    rho_Y = torch.tensor([np.array(reduced_state(y)) for y in Y])\n","    K = np.real(torch.einsum('aik,bki -> ab', rho_X, rho_Y)) # trace[rho(x)rho(y)\n","    return K # formula for the swap test\n","\n","\n","def get_functions(qubits, seed, M='Z', second_qubit=False,dim=1):\n","    \"\"\"\n","    Utility function that creates all the functionalities for a experimental setting\n","    :param qubits: number of qubits involved == dimensionality of data\n","    :param seed: random seed to generate V\n","    :param M: observable on qubit 1 that defines the target functioon\n","    :param second_qubit: If trueallso returns functions to compute the kernel matrix of q_w (2nd qubit)\n","    :return:\n","    \"\"\"\n","    dev = qml.device('default.qubit', wires=qubits)\n","    # define the full kernel\n","    k_prod_fct = lambda x, y: full_kernel_fct(x, y, data_encoding=product_encoding)\n","    k_prod = qml.QNode(k_prod_fct, dev)\n","\n","    # make everything work with the pennylane interfaces\n","\n","    # function that returns reduced state of first qubit\n","    projected_kernels = []\n","    subset_index = [[0], [1], [2], [0, 1], [0, 2], [1, 2], [0, 1, 2]]\n","    subset_index = subset_index[:dim]\n","    for i in subset_index:\n","      reduced_state_fct = lambda x,i=i: circuit_function(x, product_encoding, qubits, return_reduced_state=True, seed=seed, reduced_state=i)\n","      reduced_state = qml.QNode(reduced_state_fct, dev)\n","      # kernel matrix for biased kernel of qubit 1\n","      k_prod_bias = lambda x, y: biased_kernel_fct(x, y, reduced_state)\n","      kernel_matrix_bias1 = lambda X, Y,reduced_state=reduced_state: biased_kernel_matrix(X, Y, reduced_state)\n","      projected_kernels.append(kernel_matrix_bias1)\n","\n","    # kernel_matrix_bias = projected_kernels\n","    def kernel_matrix_bias(X,Y):\n","      res = projected_kernels[0](X,Y)\n","      for i in range(1,len(projected_kernels)):\n","        res += projected_kernels[i](X,Y)\n","      #if qubits > 1:\n","      # for i in range(qubits):\n","      #   print(f\"kernel {i}:\")\n","      #   print(projected_kernels[i](X,Y))\n","\n","      return res / len(projected_kernels)\n","\n","\n","    # target function\n","    f_fct = lambda x: circuit_function(x, product_encoding, qubits, return_reduced_state=False, seed=seed, M=M)\n","    f = qml.QNode(f_fct, dev)\n","\n","    # potentially add second qubit biased kernel functionality\n","    if second_qubit==True and qubits > 1:\n","        reduced_state_fct_2 = lambda x: circuit_function(x, product_encoding, qubits, return_reduced_state=1, seed=seed, reduced_state=1)\n","        reduced_state_2 = qml.QNode(reduced_state_fct_2, dev)\n","        k_prod_bias_2 = lambda x, y: biased_kernel_fct(x, y, reduced_state_2)\n","        kernel_matrix_bias_2 = lambda X, Y: biased_kernel_matrix(X, Y, reduced_state_2)\n","        return k_prod, k_prod_bias, f, kernel_matrix_bias, kernel_matrix_bias_2\n","    else:\n","        return k_prod, k_prod_bias, f, kernel_matrix_bias, kernel_matrix_bias\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MEdjnpJkVWDr"},"outputs":[],"source":["def feature_map_full(x):\n","    \"\"\"\n","    X: shape (N, 2) where each row is [x1, x2].\n","\n","    Returns: Phi, shape (N, 5),\n","             where Phi[i, :] = [1, cos(x1), sin(x1), cos(x2), sin(x2)] for the i-th sample.\n","    \"\"\"\n","    x0,x1,x2 = x[0],x[1],x[2]\n","    u0,u1,u2,u3 = 1/8,1/8,1/16,1/32\n","\n","    return np.array([\n","        u0**0.5 * 1,\n","\n","        u1**0.5 * np.cos(x0), u1**0.5 * np.sin(x0),\n","        u1**0.5 * np.cos(x1), u1**0.5 * np.sin(x1),\n","        u1**0.5 * np.cos(x2), u1**0.5 * np.sin(x2),\n","\n","        u2**0.5 * np.cos(x0+x1), u2**0.5 * np.sin(x0+x1), u2**0.5 * np.cos(x0-x1), u2**0.5 * np.sin(x0-x1),\n","        u2**0.5 * np.cos(x0+x2), u2**0.5 * np.sin(x0+x2), u2**0.5 * np.cos(x0-x2), u2**0.5 * np.sin(x0-x2),\n","        u2**0.5 * np.cos(x1+x2), u2**0.5 * np.sin(x1+x2), u2**0.5 * np.cos(x1-x2), u2**0.5 * np.sin(x1-x2),\n","\n","        u3**0.5 * np.cos(x0+x1+x2), u3**0.5 * np.sin(x0+x1+x2),\n","        u3**0.5 * np.cos(x0+x1-x2), u3**0.5 * np.sin(x0+x1-x2),\n","        u3**0.5 * np.cos(x0-x1+x2), u3**0.5 * np.sin(x0-x1+x2),\n","        u3**0.5 * np.cos(x0-x1-x2), u3**0.5 * np.sin(x0-x1-x2)\n","    ])\n","\n","sigma_w = 0.5\n","w_random = np.random.normal(loc=0.0, scale=sigma_w, size=27)\n","#w_random = np.array([1,1,1,1,1,1,1,1,1])\n","#w_random[7],w_random[8] = 1,1\n","print(w_random)\n","\n","def f_draw_from_GP(x):\n","    \"\"\"A random draw from the GP prior with feature map phi, weights ~ N(0, sigma^2 I).\"\"\"\n","    Phi = feature_map_full(x)\n","    return np.sum(Phi * w_random)"]},{"cell_type":"markdown","metadata":{"id":"eE07bhiyPtnN"},"source":["# repeat for different kernel dimension"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2A84WWn4f6_h"},"outputs":[],"source":["w_random = np.array([1 for _ in range(27)])\n","w_random = np.random.normal(loc=0.0, scale=sigma_w, size=27)\n","\n","\n","qubits = 3\n","seed = 1\n","bounds = [(0, 2*np.pi), (0, 2*np.pi),(0, 2*np.pi)]\n","\n","quantum_f = lambda X: np.array([f_draw_from_GP(X[i]) for i in range(X.shape[0])])\n","\n","\n","dim_tried = []\n","regret_for_dim = []\n","std_regret_for_dim = []\n","\n","for dim in trange(1,8, desc=\"Number of projected kernels Used\"):\n","\n","  _, _, f, kernel_matrix_bias, kernel_second_qubit = get_functions(\n","            qubits=3, seed=seed, second_qubit=True,dim=dim\n","        )\n","\n","  kernell = lambda X,Y: kernel_matrix_bias(X,Y).numpy()\n","\n","  gp = GaussianProcessRegressor(kernel=kernell, alpha=1e-4)\n","\n","  avg_best, best_hist_all,std_regret = run_multiple_experiments_SquareCB(\n","  f=quantum_f,\n","  gp=gp,\n","  bounds=bounds,\n","  n_runs=30,\n","  n_iter=100,\n","  init_points=1,\n","  n_grid=10,\n","  beta_func=None,  # use the default inside SquareCB_nd\n","  random_state=42,\n","  error = 6/dim,\n","  verbose=False\n",")\n","\n","  dim_tried.append(dim)\n","  regret_for_dim.append(avg_best[-1]/6)\n","  std_regret_for_dim.append(std_regret[-1]/6)\n","\n","plt.errorbar(dim_tried,regret_for_dim,std_regret_for_dim,ls=\"-\",\n","             marker='d',\n","             color=\"#009E73\",\n","             alpha=1.0,\n","             capsize=4)\n","plt.title(\"Regret for Different Kernel Used\")\n","plt.xlabel(\"Number of Projected Kernel Used for Modeling\")\n","plt.ylabel(\"Regret for T=100\")\n","plt.grid(True)\n","plt.legend()\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"3sROBMs3biIj"},"outputs":[],"source":["plt.plot(dim_tried,regret_for_dim)\n","plt.title(\"Regret for Different Kernel Used\")\n","plt.xlabel(\"Dimension of Kernel Used\")\n","plt.ylabel(\"Regret\")\n","plt.grid(True)\n","plt.legend()\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"PODnHONBBcnx"},"outputs":[],"source":["print(regret_for_dim)\n","print(std_regret_for_dim)"]}],"metadata":{"colab":{"collapsed_sections":["k_Ch3AEZr_HP"],"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}