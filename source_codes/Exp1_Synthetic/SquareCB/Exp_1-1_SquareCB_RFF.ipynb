{"cells":[{"cell_type":"markdown","metadata":{"id":"k_Ch3AEZr_HP"},"source":["# Helper Function"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ff_B0fAhKrND"},"outputs":[],"source":["\"\"\"\n","This module contains all the functions that define the kernels\n","\"\"\"\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import torch\n","\n","# The quantum kernel with pauli X encoding is identical to this, to speed up computation we used this\n","def full_kernel_classic(x, y):\n","    \"\"\"\n","    Classical evaluation of the full kernel\n","    :param x: arg 1\n","    :param y: arg 2\n","    :return:\n","    \"\"\"\n","    k = 1\n","    for i in range(len(x)):\n","        k = k * np.cos(1/2*(x[i]-y[i]))**2\n","    return k\n","\n","\n","def kernel_matrix_classic(X, Y):\n","    \"\"\"\n","    compute the kenel matrix of the full kernel\n","    :param X: vector of samples\n","    :param Y: vector of samples\n","    :return:\n","    \"\"\"\n","    K = np.zeros((len(X), len(Y)))\n","    for i in range(len(X)):\n","        for j in range(len(Y)):\n","            K[i, j] = full_kernel_classic(X[i], Y[j])\n","    return K\n","\n","\n","def kernel_matrix_classic_torch(X, Y):\n","    \"\"\"\n","    compute the kenel matrix of the full kernel potentially utilizing some parallel processing\n","    :param X: vector of samples\n","    :param Y: vector of samples\n","    :return:\n","    \"\"\"\n","    if type(X) is np.ndarray:\n","        X = torch.from_numpy(X)\n","    if type(Y) is np.ndarray:\n","        Y = torch.from_numpy(Y)\n","    # create tensor with entry i x j x k equal to x_ik - y_jk\n","    X = X.unsqueeze(1).expand(-1, Y.size(0), -1)\n","    Y = Y.unsqueeze(0).expand(X.size(0), -1, -1)\n","    K = X - Y\n","    K = torch.cos(K / 2) ** 2\n","    K = torch.prod(K, 2)\n","    return K\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5ULe1zRLh8-I"},"outputs":[],"source":["import numpy as np\n","\n","class GaussianProcessRegressor:\n","    \"\"\"\n","    Gaussian Process Regressor for d-dimensional inputs.\n","    Returns both predictive mean and std if requested.\n","    \"\"\"\n","    def __init__(self, kernel, alpha=1e-5):\n","        self.kernel = kernel\n","        self.alpha = alpha\n","        self.X_train = None\n","        self.y_train = None\n","        self.K_inv = None\n","\n","    def fit(self, X, y):\n","        X = np.array(X)\n","        y = np.array(y)\n","        self.X_train = X\n","        self.y_train = y\n","        K = self.kernel(X, X)\n","        K += self.alpha * np.eye(len(X))\n","        self.K_inv = np.linalg.inv(K)\n","\n","    def predict(self, X_test, return_std=False):\n","\n","        X_test = np.array(X_test)\n","        K_star = self.kernel(X_test, self.X_train)\n","        y_mean = K_star @ (self.K_inv @ self.y_train)\n","\n","        if return_std:\n","            K_star_star = self.kernel(X_test, X_test)\n","            cov = K_star_star - K_star @ self.K_inv @ K_star.T\n","            var = np.diag(cov)\n","            var = np.maximum(var, 0.0)\n","            y_std = np.sqrt(var)\n","            return y_mean, y_std\n","        else:\n","            return y_mean\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6EkvzaE6bmOU"},"outputs":[],"source":["from itertools import product\n","\n","def build_grid(bounds, n_grid):\n","    \"\"\"\n","    Build a uniform grid of points within the given 'bounds'.\n","\n","    Parameters\n","    ----------\n","    bounds : list of (low, high) for each dimension (length d)\n","    n_grid : int\n","        Number of grid points per dimension.\n","\n","    Returns\n","    -------\n","    X_grid : np.ndarray of shape (n_grid^d, d)\n","        All points in the grid.\n","    \"\"\"\n","    # For each dimension, create an array of n_grid points from low to high\n","    axes = [np.linspace(low, high, n_grid) for (low, high) in bounds]\n","    # Cartesian product of all axes\n","    # e.g. for d=2, we get all pairs (x,y); for d=3, all (x,y,z), etc.\n","    mesh = list(product(*axes))  # a list of d-tuples\n","    return np.array(mesh)\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zUirsK3YG8nr"},"outputs":[],"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","from itertools import product\n","from scipy.stats import norm\n","\n","\n","\n","\n","\n","def SquareCB_nd(\n","    f,                 # black-box reward function, shape: (n, d) -> (n,)\n","    gp,                # a GaussianProcessRegressor (our \"oracle\")\n","    bounds,            # list of (low, high) for each dimension\n","    n_iter=100,         # number of SquareCB iterations\n","    init_points=3,     # number of initial random samples\n","    n_grid=5,         # discrete set of arms = the grid\n","    gamma=1.0,         # SquareCB learning-rate parameter\n","    mu=None,           # exploration offset, default = K (the number of grid points)\n","    random_state=None,\n","    noise_std=0.1,     # optional: noise level for synthetic reward observation\n","    verbose=False,\n","    beta_func = None,\n","    error = 1\n","):\n","    \"\"\"\n","    Illustrative 'SquareCB' variant for Bayesian Opt. with a GP 'oracle' + discrete grid.\n","\n","    Parameters\n","    ----------\n","    f : callable\n","        The unknown reward function mapping array (n,d)->(n,).\n","    gp : GaussianProcessRegressor\n","        Must have .fit(X, y) and .predict(X, return_std=...) etc.\n","        We treat gp as our 'online regression' oracle for the paper's SquareCB.\n","    bounds : list of (low, high)\n","        The domain bounding box in d dimensions.\n","    n_iter : int\n","        Number of SquareCB rounds.\n","    init_points : int\n","        How many random points to sample initially (random exploration).\n","    n_grid : int\n","        Grid resolution in each dimension.  => K = n_grid^d arms.\n","    gamma : float\n","        The 'learning rate' parameter from SquareCB.  Usually tuned or derived from theory.\n","    mu : float or None\n","        Exploration offset.  If None, we set mu = K = n_grid^d.\n","    random_state : int or None\n","        For reproducibility.\n","    noise_std : float\n","        Optional noise standard deviation for the observation of f(x).\n","    verbose : bool\n","        Print iteration details if True.\n","\n","    Returns\n","    -------\n","    regrets : ndarray of shape (n_iter + init_points,)\n","        Cumulative regret at each iteration.\n","    X_samples : ndarray of shape (n_iter + init_points, d)\n","        The chosen points (arms) each round.\n","    Y_samples : ndarray of shape (n_iter + init_points,)\n","        The noisy observed rewards from f.\n","    \"\"\"\n","\n","    if random_state is not None:\n","        np.random.seed(random_state)\n","\n","    d = len(bounds)\n","    # 1) Build the discrete action set via a grid\n","    X_grid = build_grid(bounds, n_grid=n_grid)   # shape (K, d)\n","    K = X_grid.shape[0]                          # total number of arms\n","\n","    if mu is None:\n","        mu = 1#float(K)  # default per the paper's suggestion is mu = #actions\n","\n","    # 2) Approximate global max among the grid for regret computation\n","    Y_grid = f(X_grid)\n","    idx_best = np.argmax(Y_grid)\n","    x_star = X_grid[idx_best]\n","    f_star = Y_grid[idx_best]   # approximate best reward among grid\n","\n","    # 3) Initialize data with some random points from domain\n","    #    (like in your GP-UCB code)\n","    def sample_random(n):\n","        return np.array([\n","            [np.random.uniform(low, high) for (low, high) in bounds]\n","            for _ in range(n)\n","        ])\n","    X_samples = sample_random(init_points)            # (init_points, d)\n","    Y_samples = f(X_samples) + noise_std * np.random.randn(init_points)  # add noise\n","\n","    # 4) Track cumulative regrets\n","    regrets = np.zeros(n_iter + init_points)\n","    cum_regret = 0.0\n","    for i in range(init_points):\n","        cum_regret += (f_star - Y_samples[i])\n","        regrets[i] = cum_regret\n","\n","    # 5) Main loop: 'SquareCB'\n","    for step in range(n_iter):\n","        t = init_points + step + 1  # 1-based iteration index\n","\n","        # (a) Fit the GP on current data (X_samples, Y_samples)\n","        gp.fit(X_samples, Y_samples)\n","\n","        # (b) Predict the reward for each action a in X_grid\n","        #     SquareCB uses \"predicted losses\" but we can equivalently treat negative reward\n","        #     or just pick an \"argmin predicted\" approach. We'll treat \"loss = - predicted_reward.\"\n","        mean_grid, std_grid = gp.predict(X_grid, return_std=True)\n","\n","        # In classical SquareCB, we want a \"score\" = predicted loss.\n","        # If we only have predicted reward, we can define:\n","        #   y_hat[a] = -mean_grid[a]\n","        # i.e. the \"predicted loss\" = negative predicted reward\n","        y_hat_grid = -mean_grid\n","\n","        # (c) Identify 'best action' b_t = arg min y_hat\n","        b_idx = np.argmin(y_hat_grid)\n","        y_hat_b = y_hat_grid[b_idx]\n","\n","        # (d) For each a != b, define p_t(a) = 1 / [mu + gamma*(y_hat[a] - y_hat_b)]\n","        #     Then p_t(b) = 1 - sum_{a!=b} p_t(a).\n","\n","\n","        mu = 100#K\n","        gamma =100# ( K * n_iter / error ) ** 0.5 * 5\n","\n","        p = np.zeros(K, dtype=float)\n","        sum_others = 0.0\n","        for a_idx in range(K):\n","            if a_idx != b_idx:\n","                gap = (y_hat_grid[a_idx] - y_hat_b)\n","                # If gap <= 0, it can be tricky or cause big p. Add small offset if needed\n","                denom = mu + gamma*max(0.0, gap)\n","                p[a_idx] = 1.0 / denom\n","                sum_others += p[a_idx]\n","        #print(sum_others)\n","        p[b_idx] = 1.0 - sum_others\n","        if p[b_idx] < 0.0:\n","            # If numerical issues occur, clip or renormalize\n","            p = np.clip(p, 0, None)\n","            p /= np.sum(p)\n","\n","        # (e) Sample next action x_next from distribution p\n","        #print(f(X_grid),mean_grid,p)\n","        #print(f\"====={step}======\")\n","        a_next = np.random.choice(K, p=p)\n","        x_next = X_grid[a_next].reshape(1, -1)\n","\n","        # (f) Observe reward from f with optional noise\n","        y_true = f(x_next)  # shape (1,)\n","        noise = noise_std * np.random.randn()\n","        y_obs = y_true[0] + noise\n","\n","        # (g) Update data\n","        X_samples = np.vstack([X_samples, x_next])\n","        Y_samples = np.concatenate([Y_samples, [y_obs]])\n","\n","        # (h) Update cumulative regret\n","        cum_regret += (f_star - y_true[0])\n","        regrets[init_points + step] = cum_regret\n","\n","        if verbose:\n","            print(f\"[SquareCB] Round {step+1}/{n_iter}, t={t}, bestArm={b_idx}, \"\n","                  f\"ChosenArm={a_next}, regret={cum_regret:.4f}\")\n","\n","    return regrets, X_samples, Y_samples\n","\n","\n","\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from tqdm import trange\n","\n","def run_multiple_experiments_SquareCB(\n","    f,\n","    gp,\n","    bounds,\n","    n_runs=5,\n","    n_iter=10,\n","    init_points=3,\n","    n_grid=50,\n","    beta_func=None,\n","    random_state=None,\n","    error = 1,\n","    verbose=False\n","):\n","    \"\"\"\n","    Runs the GP-UCB experiment 'n_runs' times, each time creating a new GP instance.\n","    Returns the average cumulative regret across runs.\n","\n","    Parameters\n","    ----------\n","    f : callable\n","        The black-box reward function.\n","    gp_class_factory : callable\n","        A function that, when called, returns a *new* untrained GaussianProcessRegressor\n","        (or similar). For example:\n","          lambda: GaussianProcessRegressor(kernel=RBF(...), alpha=..., optimizer=None)\n","        We need a fresh GP for each run, so that each run is independent.\n","    bounds : list of (low, high)\n","        Domain bounding box.\n","    n_runs : int\n","        Number of independent runs to average over.\n","    n_iter : int\n","        Number of GP-UCB rounds (not counting the init_points).\n","    init_points : int\n","        Number of random initial points in each run.\n","    n_grid : int\n","        Grid resolution for argmax search in each run.\n","    beta_func : callable or None\n","        If None, use a default log-based. Otherwise a function beta_func(t) -> float.\n","    random_state : int or None\n","        For reproducibility. If set, seeds the first run's RNG, then subsequent runs\n","        will shift the seed.\n","    verbose : bool\n","        Whether to print details for each run.\n","\n","    Returns\n","    -------\n","    avg_regret : ndarray of shape (n_iter + init_points,)\n","        The pointwise average of the cumulative regret across runs.\n","    regrets_all : ndarray of shape (n_runs, n_iter + init_points)\n","        The individual run's cumulative-regret curves.\n","    \"\"\"\n","\n","    from copy import deepcopy\n","\n","    # If we want reproducibility, set base seed\n","    base_seed = random_state if random_state is not None else None\n","\n","    # We'll store the regret curve for each run here\n","    regrets_all = []\n","\n","    for i in trange(n_runs, desc=\"GP-UCB Experiments\"):\n","        # For each run, optionally shift the seed\n","        if base_seed is not None:\n","            # shift by i to get distinct seeds\n","            np.random.seed(base_seed + i)\n","\n","        # Create a fresh GP instance\n","        gp_model = gp\n","\n","        # Import or copy the SquareCB_nd from your previous code snippet:\n","        regrets, X_samples, Y_samples = SquareCB_nd(\n","            f=f,\n","            gp=gp_model,\n","            bounds=bounds,\n","            n_iter=n_iter,\n","            init_points=init_points,\n","            n_grid=n_grid,\n","            beta_func=beta_func,\n","            random_state=None,  # we've set the seed externally\n","            error = error,\n","            verbose=(verbose and i == 0)  # only verbose in the 1st run, e.g.\n","        )\n","\n","        regrets_all.append(regrets)\n","\n","    regrets_all = np.array(regrets_all)  # shape (n_runs, n_steps)\n","    avg_regret = regrets_all.mean(axis=0)\n","    std_regret = regrets_all.std(axis=0)\n","    return avg_regret, regrets_all, std_regret\n","\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GwQLCcYwLbuj"},"outputs":[],"source":["lst_of_c = np.array([1/8, 1/8,1/8,1/8,1/8,1/8,1/8,\n","          1/16,1/16,1/16,1/16,1/16,1/16,1/16,1/16,1/16,1/16,1/16,1/16,\n","          1/32,1/32,1/32,1/32,1/32,1/32,1/32,1/32,\n","          ])\n","lst_of_sqrt_c = lst_of_c**0.5\n","\n","# lst_of_features = [lambda d0,d1,d2: np.cos(d0)/8, lambda d0,d1,d2:np.cos(d1)/8, lambda d0,d1,d2:np.cos(d2)/8,\n","#             lambda d0,d1,d2:np.cos(d0+d1)/16, lambda d0,d1,d2:np.cos(d0+d2)/16, lambda d0,d1,d2:np.cos(d1+d2)/16,\n","#             lambda d0,d1,d2:np.cos(d0-d1)/16, lambda d0,d1,d2:np.cos(d0-d2)/16, lambda d0,d1,d2:np.cos(d1-d2)/16,\n","#             lambda d0,d1,d2:np.cos(d0+d1+d2)/32, lambda d0,d1,d2:np.cos(d0+d1-d2)/32, lambda d0,d1,d2:np.cos(d0-d1+d2)/32, lambda d0,d1,d2:np.cos(d0-d1-d2)/32\n","#           ]\n","\n","def feature_map_full(x):\n","    \"\"\"\n","    X: shape (N, 2) where each row is [x1, x2].\n","\n","    Returns: Phi, shape (N, 5),\n","             where Phi[i, :] = [1, cos(x1), sin(x1), cos(x2), sin(x2)] for the i-th sample.\n","    \"\"\"\n","    x0,x1,x2 = x[0],x[1],x[2]\n","    u0,u1,u2,u3 = 1/8,1/8,1/16,1/32\n","\n","    return np.array([\n","        u0**0.5 * 1,\n","\n","        u1**0.5 * np.cos(x0), u1**0.5 * np.sin(x0),\n","        u1**0.5 * np.cos(x1), u1**0.5 * np.sin(x1),\n","        u1**0.5 * np.cos(x2), u1**0.5 * np.sin(x2),\n","\n","        u2**0.5 * np.cos(x0+x1), u2**0.5 * np.sin(x0+x1), u2**0.5 * np.cos(x0-x1), u2**0.5 * np.sin(x0-x1),\n","        u2**0.5 * np.cos(x0+x2), u2**0.5 * np.sin(x0+x2), u2**0.5 * np.cos(x0-x2), u2**0.5 * np.sin(x0-x2),\n","        u2**0.5 * np.cos(x1+x2), u2**0.5 * np.sin(x1+x2), u2**0.5 * np.cos(x1-x2), u2**0.5 * np.sin(x1-x2),\n","\n","        u3**0.5 * np.cos(x0+x1+x2), u3**0.5 * np.sin(x0+x1+x2),\n","        u3**0.5 * np.cos(x0+x1-x2), u3**0.5 * np.sin(x0+x1-x2),\n","        u3**0.5 * np.cos(x0-x1+x2), u3**0.5 * np.sin(x0-x1+x2),\n","        u3**0.5 * np.cos(x0-x1-x2), u3**0.5 * np.sin(x0-x1-x2)\n","    ])\n","\n","def reduced_kernel_classic(x, y, dim=27,seed=None):\n","    \"\"\"\n","    Classical evaluation of the full kernel\n","    :param x: arg 1\n","    :param y: arg 2\n","    :return:\n","    \"\"\"\n","    x_feats = feature_map_full(x)\n","    y_feats = feature_map_full(y)\n","    rng = np.random.default_rng(seed)\n","    x_feats = rng.permutation(x_feats)[:dim]       # returns a new NumPy array\n","    rng = np.random.default_rng(seed)\n","    y_feats = rng.permutation(y_feats)[:dim]\n","\n","    return np.sum(x_feats * y_feats)\n","\n","\n","def reduced_matrix_classic(X, Y, dim=27,seed=None):\n","    \"\"\"\n","    compute the kenel matrix of the full kernel\n","    :param X: vector of samples\n","    :param Y: vector of samples\n","    :return:\n","    \"\"\"\n","    #print(dim)\n","\n","    K = np.zeros((len(X), len(Y)))\n","    for i in range(len(X)):\n","        for j in range(len(Y)):\n","            K[i, j] = reduced_kernel_classic(X[i], Y[j],dim,seed=seed)\n","    return K\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"ehDK46gjM89W"},"outputs":[],"source":["# bounds = [(0, 2*np.pi), (0, 2*np.pi),(0, 2*np.pi)]\n","# X_grid = build_grid(bounds, n_grid=3)\n","# for i in range(1,28,2):\n","#   print(i)\n","#   print(kernel_matrix_classic(X_grid,X_grid)[10,10] - reduced_matrix_classic(X_grid,X_grid,i,seed=1)[10,10])\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MEdjnpJkVWDr"},"outputs":[],"source":["sigma_w = 0.5\n","w_random = np.random.normal(loc=0.0, scale=sigma_w, size=27)\n","#w_random = np.array([1,1,1,1,1,1,1,1,1])\n","#w_random[7],w_random[8] = 1,1\n","print(w_random)\n","\n","def f_draw_from_GP(x):\n","    \"\"\"A random draw from the GP prior with feature map phi, weights ~ N(0, sigma^2 I).\"\"\"\n","    Phi = feature_map_full(x)\n","    return np.sum(Phi * w_random)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZTlDa3qYW3GI"},"outputs":[],"source":["# x = [0,0.44,-1]\n","# y = [-3,2,0.3]\n","# full_kernel_classic(x,y)\n","\n","# sum(feature_map_full(x) * feature_map_full(y))\n","# w_random = np.random.normal(loc=0.0, scale=sigma_w, size=27)\n","# f_draw_from_GP(x)"]},{"cell_type":"markdown","metadata":{"id":"eE07bhiyPtnN"},"source":["# repeat for different kernel dimension"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0yNpB4ujKsGY"},"outputs":[],"source":["import random\n","\n","w_random = np.array([1 for _ in range(27)])\n","w_random = np.random.normal(loc=0.0, scale=sigma_w, size=27)\n","\n","\n","qubits = 3\n","seed = 1\n","bounds = [(0, 2*np.pi), (0, 2*np.pi),(0, 2*np.pi)]\n","\n","quantum_f = lambda X: np.array([f_draw_from_GP(X[i]) for i in range(X.shape[0])])\n","\n","\n","dim_tried = []\n","regret_for_dim = []\n","std_regret_for_dim = []\n","\n","for dim in trange(1,28,1, desc=\"Dimension of Kernel Used\"):\n","\n","  kernell = lambda X,Y: reduced_matrix_classic(X,Y,dim=dim,seed=random.getrandbits(10))\n","\n","  gp = GaussianProcessRegressor(kernel=kernell, alpha=1e-4)\n","\n","  avg_best, best_hist_all,std_regret = run_multiple_experiments_SquareCB(\n","  f=quantum_f,\n","  gp=gp,\n","  bounds=bounds,\n","  n_runs=30,\n","  n_iter=100,\n","  init_points=1,\n","  n_grid=10,\n","  beta_func=None,  # use the default inside SquareCB_nd\n","  random_state=42,\n","  error = 6/dim**0.5,#get_max_error(dim),\n","  verbose=False\n",")\n","\n","  dim_tried.append(dim)\n","  regret_for_dim.append(avg_best[-1]/6)\n","  std_regret_for_dim.append(std_regret[-1]/6)\n","  print(avg_best)\n","\n","\n","plt.errorbar(dim_tried,regret_for_dim,std_regret_for_dim,ls=\"-\",\n","             marker='d',\n","             color=\"#009E73\",\n","             alpha=1.0,\n","             capsize=4)\n","plt.title(\"Regret for Different Kernel Used\")\n","plt.xlabel(\"Dimension of Kernel Used for Modeling\")\n","plt.ylabel(\"Regret for T=100\")\n","plt.grid(True)\n","plt.legend()\n","plt.show()\n","\n","\n","print(regret_for_dim)\n","print(std_regret_for_dim)"]}],"metadata":{"colab":{"collapsed_sections":["k_Ch3AEZr_HP"],"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}